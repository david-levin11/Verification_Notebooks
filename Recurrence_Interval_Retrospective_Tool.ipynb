{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/david-levin11/Verification_Notebooks/blob/main/Recurrence_Interval_Retrospective_Tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iYyzL_9EAm7"
      },
      "source": [
        "# **Alaska Region Precipitation & Recurrence Interval Retrospective Tool**\n",
        "<br/>\n",
        "Description--This tool will update the ArcGIS Online retrospective version of the Alaska Region Precipitation & Recurrence Interval tool to look at past cases/events.\n",
        "\n",
        "- David Levin, Arctic Testbed & Proving Ground, Anchorage Alaska"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t56SexSlUELy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNRHFuNmC-2K"
      },
      "source": [
        "##**1 - Install and Import Packages**\n",
        "This will take about a minute to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJaogqYCNB-Q",
        "outputId": "37db0f3b-8220-4dc3-cc3a-8128db31ca40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arcgis\n",
            "  Using cached arcgis-2.3.1.tar.gz (46.9 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from arcgis) (9.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.1 in /usr/local/lib/python3.10/dist-packages (from arcgis) (2.0.7)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from arcgis) (5.5.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from arcgis) (4.9.4)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from arcgis) (6.5.5)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from arcgis) (43.0.0)\n",
            "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.10/dist-packages (from arcgis) (7.7.1)\n",
            "Requirement already satisfied: widgetsnbextension>=3 in /usr/local/lib/python3.10/dist-packages (from arcgis) (3.6.8)\n",
            "Requirement already satisfied: jupyter-client<=6.1.12 in /usr/local/lib/python3.10/dist-packages (from arcgis) (6.1.12)\n",
            "Requirement already satisfied: pandas<2.2.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from arcgis) (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from arcgis) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from arcgis) (3.7.1)\n",
            "Requirement already satisfied: keyring>=23.3.0 in /usr/lib/python3/dist-packages (from arcgis) (23.5.0)\n",
            "Collecting pylerc (from arcgis)\n",
            "  Using cached pylerc-4.0-py3-none-any.whl\n",
            "Collecting ujson>=3 (from arcgis)\n",
            "  Using cached ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting jupyterlab (from arcgis)\n",
            "  Using cached jupyterlab-4.2.5-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pyshp>=2 in /usr/local/lib/python3.10/dist-packages (from arcgis) (2.3.1)\n",
            "Collecting geomet (from arcgis)\n",
            "  Using cached geomet-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting requests<2.32.0,>=2.30.0 (from arcgis)\n",
            "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from arcgis) (1.3.1)\n",
            "Collecting requests_toolbelt (from arcgis)\n",
            "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pyspnego>=0.8.0 (from arcgis)\n",
            "  Using cached pyspnego-0.11.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting requests-kerberos (from arcgis)\n",
            "  Using cached requests_kerberos-0.15.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting requests-gssapi (from arcgis)\n",
            "  Using cached requests_gssapi-1.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: dask>=2023.3.2 in /usr/local/lib/python3.10/dist-packages (from arcgis) (2024.7.1)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from arcgis) (0.1.7)\n",
            "Requirement already satisfied: pyarrow>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from arcgis) (14.0.2)\n",
            "Collecting puremagic<2,>=1.15 (from arcgis)\n",
            "  Using cached puremagic-1.27-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting truststore>=0.7.0 (from arcgis)\n",
            "  Using cached truststore-0.9.2-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask>=2023.3.2->arcgis) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2023.3.2->arcgis) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2023.3.2->arcgis) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2023.3.2->arcgis) (24.1)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2023.3.2->arcgis) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask>=2023.3.2->arcgis) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2023.3.2->arcgis) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2023.3.2->arcgis) (8.4.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->arcgis) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->arcgis) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->arcgis) (5.7.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->arcgis) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->arcgis) (3.0.13)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-client<=6.1.12->arcgis) (5.7.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client<=6.1.12->arcgis) (24.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client<=6.1.12->arcgis) (2.8.2)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client<=6.1.12->arcgis) (6.3.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.0,>=2.0.0->arcgis) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.0,>=2.0.0->arcgis) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<2.32.0,>=2.30.0->arcgis) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<2.32.0,>=2.30.0->arcgis) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<2.32.0,>=2.30.0->arcgis) (2024.7.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook->arcgis) (3.1.4)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->arcgis) (23.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook->arcgis) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook->arcgis) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->arcgis) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->arcgis) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->arcgis) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->arcgis) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->arcgis) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->arcgis) (1.17.0)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->arcgis)\n",
            "  Using cached async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting httpx>=0.25.0 (from jupyterlab->arcgis)\n",
            "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting ipykernel>=4.5.1 (from ipywidgets<8,>=7->arcgis)\n",
            "  Using cached ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->arcgis)\n",
            "  Using cached jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->arcgis)\n",
            "  Using cached jupyter_server-2.14.2-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->arcgis)\n",
            "  Using cached jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->arcgis) (0.2.4)\n",
            "Requirement already satisfied: setuptools>=40.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->arcgis) (71.0.4)\n",
            "Requirement already satisfied: tomli>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->arcgis) (2.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->arcgis) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->arcgis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->arcgis) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->arcgis) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->arcgis) (3.1.2)\n",
            "Collecting gssapi (from requests-gssapi->arcgis)\n",
            "  Using cached gssapi-1.8.3-cp310-cp310-linux_x86_64.whl\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->arcgis) (3.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from async-lru>=1.0.0->jupyterlab->arcgis) (4.12.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->arcgis) (2.22)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab->arcgis) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx>=0.25.0->jupyterlab->arcgis)\n",
            "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab->arcgis) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.25.0->jupyterlab->arcgis)\n",
            "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask>=2023.3.2->arcgis) (3.20.0)\n",
            "Collecting comm>=0.1.1 (from ipykernel>=4.5.1->ipywidgets<8,>=7->arcgis)\n",
            "  Using cached comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->arcgis) (1.6.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->arcgis) (5.9.5)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets<8,>=7->arcgis)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<8,>=7->arcgis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<8,>=7->arcgis) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<8,>=7->arcgis) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<8,>=7->arcgis) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<8,>=7->arcgis) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<8,>=7->arcgis) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->notebook->arcgis) (2.1.5)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.0->jupyter-client<=6.1.12->arcgis) (4.2.2)\n",
            "INFO: pip is looking at multiple versions of jupyter-server to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->arcgis)\n",
            "  Downloading jupyter_server-2.14.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Downloading jupyter_server-2.14.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Downloading jupyter_server-2.13.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Downloading jupyter_server-2.12.5-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Downloading jupyter_server-2.12.4-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Downloading jupyter_server-2.12.3-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Downloading jupyter_server-2.12.2-py3-none-any.whl.metadata (8.4 kB)\n",
            "INFO: pip is still looking at multiple versions of jupyter-server to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading jupyter_server-2.12.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Downloading jupyter_server-2.12.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Downloading jupyter_server-2.11.2-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Downloading jupyter_server-2.11.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Downloading jupyter_server-2.10.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading jupyter_server-2.10.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Downloading jupyter_server-2.9.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "  Downloading jupyter_server-2.9.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "  Downloading jupyter_server-2.8.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "  Downloading jupyter_server-2.7.3-py3-none-any.whl.metadata (8.6 kB)\n",
            "  Downloading jupyter_server-2.7.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "  Downloading jupyter_server-2.7.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "  Downloading jupyter_server-2.7.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "  Downloading jupyter_server-2.6.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "  Downloading jupyter_server-2.5.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "  Downloading jupyter_server-2.4.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->arcgis)\n",
            "  Downloading jupyter_lsp-2.2.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ipykernel>=4.5.1 (from ipywidgets<8,>=7->arcgis)\n",
            "  Downloading ipykernel-6.29.4-py3-none-any.whl.metadata (6.3 kB)\n",
            "  Downloading ipykernel-6.29.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "  Downloading ipykernel-6.29.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading ipykernel-6.29.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading ipykernel-6.29.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading ipykernel-6.28.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading ipykernel-6.27.1-py3-none-any.whl.metadata (6.3 kB)\n",
            "  Downloading ipykernel-6.26.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "  Downloading ipykernel-6.25.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "  Downloading ipykernel-6.25.1-py3-none-any.whl.metadata (6.3 kB)\n",
            "  Downloading ipykernel-6.25.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "  Downloading ipykernel-6.24.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "  Downloading ipykernel-6.23.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "  Downloading ipykernel-6.23.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "  Downloading ipykernel-6.23.1-py3-none-any.whl.metadata (6.3 kB)\n",
            "  Downloading ipykernel-6.23.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "  Downloading ipykernel-6.22.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "  Downloading ipykernel-6.21.3-py3-none-any.whl.metadata (6.4 kB)\n",
            "  Downloading ipykernel-6.21.2-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading ipykernel-6.21.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading ipykernel-6.20.2-py3-none-any.whl.metadata (6.7 kB)\n",
            "  Downloading ipykernel-6.20.1-py3-none-any.whl.metadata (6.7 kB)\n",
            "  Downloading ipykernel-6.19.4-py3-none-any.whl.metadata (6.4 kB)\n",
            "  Downloading ipykernel-6.19.3-py3-none-any.whl.metadata (6.4 kB)\n",
            "  Downloading ipykernel-6.19.2-py3-none-any.whl.metadata (6.7 kB)\n",
            "  Downloading ipykernel-6.19.1-py3-none-any.whl.metadata (6.7 kB)\n",
            "  Downloading ipykernel-6.17.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "  Downloading ipykernel-6.17.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "  Downloading ipykernel-6.16.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "  Downloading ipykernel-6.16.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "  Downloading ipykernel-6.16.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "  Downloading ipykernel-6.15.3-py3-none-any.whl.metadata (5.4 kB)\n",
            "  Downloading ipykernel-6.15.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "  Downloading ipykernel-6.15.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "  Downloading ipykernel-6.15.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "  Downloading ipykernel-6.14.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "  Downloading ipykernel-6.13.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "  Downloading ipykernel-6.13.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading ipykernel-6.12.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading ipykernel-6.12.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading ipykernel-6.11.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading ipykernel-6.10.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading ipykernel-6.9.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading ipykernel-6.9.1-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading ipykernel-6.9.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading ipykernel-6.8.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading ipykernel-6.7.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading ipykernel-6.6.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading ipykernel-6.6.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading ipykernel-6.5.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading ipykernel-6.5.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting jupyterlab (from arcgis)\n",
            "  Downloading jupyterlab-4.2.4-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.2.3-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.2.2-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.2.1-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.2.0-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.1.8-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.1.7-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.1.6-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.1.5-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.1.4-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.1.3-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.1.2-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.1.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.0.13-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.0.12-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.0.11-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.0.10-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.0.9-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.0.8-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.0.7-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.0.6-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.0.5-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.0.4-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.0.3-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.0.2-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.0.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-4.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading jupyterlab-3.6.8-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->arcgis) (1.24.0)\n",
            "Collecting jupyter-ydoc~=0.2.4 (from jupyterlab->arcgis)\n",
            "  Downloading jupyter_ydoc-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting jupyter-server-ydoc~=0.8.0 (from jupyterlab->arcgis)\n",
            "  Downloading jupyter_server_ydoc-0.8.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->arcgis) (1.8.0)\n",
            "Collecting jupyter-server-fileid<1,>=0.6.0 (from jupyter-server-ydoc~=0.8.0->jupyterlab->arcgis)\n",
            "  Downloading jupyter_server_fileid-0.9.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting ypy-websocket<0.9.0,>=0.8.2 (from jupyter-server-ydoc~=0.8.0->jupyterlab->arcgis)\n",
            "  Downloading ypy_websocket-0.8.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting y-py<0.7.0,>=0.6.0 (from jupyter-ydoc~=0.2.4->jupyterlab->arcgis)\n",
            "  Downloading y_py-0.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->arcgis) (2.16.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->arcgis)\n",
            "  Using cached json5-0.9.25-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->arcgis) (4.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->arcgis) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->arcgis) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->arcgis) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->arcgis) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->arcgis) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->arcgis) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->arcgis) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->arcgis) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->arcgis) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook->arcgis) (2.20.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask>=2023.3.2->arcgis) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client<=6.1.12->arcgis) (1.16.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->arcgis) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->arcgis) (21.2.0)\n",
            "Collecting krb5>=0.3.0 (from pyspnego[kerberos]->requests-kerberos->arcgis)\n",
            "  Using cached krb5-0.6.0-cp310-cp310-linux_x86_64.whl\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->arcgis) (1.2.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets<8,>=7->arcgis) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->arcgis) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->arcgis) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->arcgis) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->arcgis) (0.20.0)\n",
            "Collecting jupyter-events>=0.5.0 (from jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->arcgis)\n",
            "  Using cached jupyter_events-0.10.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets<8,>=7->arcgis) (0.2.13)\n",
            "Collecting aiofiles<23,>=22.1.0 (from ypy-websocket<0.9.0,>=0.8.2->jupyter-server-ydoc~=0.8.0->jupyterlab->arcgis)\n",
            "  Downloading aiofiles-22.1.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting aiosqlite<1,>=0.17.0 (from ypy-websocket<0.9.0,>=0.8.2->jupyter-server-ydoc~=0.8.0->jupyterlab->arcgis)\n",
            "  Downloading aiosqlite-0.20.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook->arcgis) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook->arcgis) (0.5.1)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->arcgis)\n",
            "  Using cached python_json_logger-2.0.7-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->arcgis)\n",
            "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->arcgis)\n",
            "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->arcgis)\n",
            "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->arcgis)\n",
            "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->arcgis)\n",
            "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->arcgis)\n",
            "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->arcgis) (24.8.0)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->arcgis)\n",
            "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->arcgis)\n",
            "  Using cached types_python_dateutil-2.9.0.20240821-py3-none-any.whl.metadata (1.9 kB)\n",
            "Using cached puremagic-1.27-py3-none-any.whl (40 kB)\n",
            "Using cached pyspnego-0.11.1-py3-none-any.whl (130 kB)\n",
            "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Using cached truststore-0.9.2-py3-none-any.whl (17 kB)\n",
            "Using cached ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "Using cached geomet-1.1.0-py3-none-any.whl (31 kB)\n",
            "Downloading jupyterlab-3.6.8-py3-none-any.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached requests_gssapi-1.3.0-py3-none-any.whl (12 kB)\n",
            "Using cached requests_kerberos-0.15.0-py2.py3-none-any.whl (12 kB)\n",
            "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Downloading jupyter_server_ydoc-0.8.0-py3-none-any.whl (11 kB)\n",
            "Downloading jupyter_ydoc-0.2.5-py3-none-any.whl (6.2 kB)\n",
            "Using cached jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "Using cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Using cached json5-0.9.25-py3-none-any.whl (30 kB)\n",
            "Downloading jupyter_server_fileid-0.9.2-py3-none-any.whl (16 kB)\n",
            "Downloading y_py-0.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ypy_websocket-0.8.4-py3-none-any.whl (10 kB)\n",
            "Downloading aiofiles-22.1.0-py3-none-any.whl (14 kB)\n",
            "Downloading aiosqlite-0.20.0-py3-none-any.whl (15 kB)\n",
            "Using cached jupyter_events-0.10.0-py3-none-any.whl (18 kB)\n",
            "Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
            "Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "Using cached types_python_dateutil-2.9.0.20240821-py3-none-any.whl (9.7 kB)\n",
            "Building wheels for collected packages: arcgis\n",
            "  Building wheel for arcgis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for arcgis: filename=arcgis-2.3.1-cp310-cp310-linux_x86_64.whl size=8729060 sha256=7914c7c616eeb835961e4541cdfb9a94acbfca62089049596a50432d48f3f736\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/20/9e/c2739322c2731ffe5caaa65164e3bd4cdac598f86b0fee2dac\n",
            "Successfully built arcgis\n",
            "Installing collected packages: y-py, puremagic, uri-template, ujson, types-python-dateutil, truststore, rfc3986-validator, rfc3339-validator, requests, python-json-logger, pylerc, krb5, jupyter-ydoc, jsonpointer, json5, jedi, gssapi, geomet, fqdn, aiosqlite, aiofiles, ypy-websocket, requests_toolbelt, requests-gssapi, arrow, pyspnego, isoduration, requests-kerberos, jupyter-events, jupyterlab-server, jupyter-server-fileid, jupyter-server-ydoc, jupyterlab, arcgis\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-22.1.0 aiosqlite-0.20.0 arcgis-2.3.1 arrow-1.3.0 fqdn-1.5.1 geomet-1.1.0 gssapi-1.8.3 isoduration-20.11.0 jedi-0.19.1 json5-0.9.25 jsonpointer-3.0.0 jupyter-events-0.10.0 jupyter-server-fileid-0.9.2 jupyter-server-ydoc-0.8.0 jupyter-ydoc-0.2.5 jupyterlab-3.6.8 jupyterlab-server-2.27.3 krb5-0.6.0 puremagic-1.27 pylerc-4.0 pyspnego-0.11.1 python-json-logger-2.0.7 requests-2.31.0 requests-gssapi-1.3.0 requests-kerberos-0.15.0 requests_toolbelt-1.0.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 truststore-0.9.2 types-python-dateutil-2.9.0.20240821 ujson-5.10.0 uri-template-1.3.0 y-py-0.6.2 ypy-websocket-0.8.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "!pip install arcgis\n",
        "\n",
        "import os\n",
        "import urllib\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import ast\n",
        "from datetime import datetime\n",
        "from arcgis.gis import GIS\n",
        "from arcgis.features import FeatureLayerCollection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2. Update Point ARI Data**\n",
        "If the ARI file exists on CMS this will take milliseconds.  If it does not exist the file will have to be created on the fly.  If this is the case the average run time is around 35 minutes.  Thankfully you would only have to do this one time per session!\n",
        "\n"
      ],
      "metadata": {
        "id": "-cr99dD1FLeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@markdown **Enter your Synoptic API token below**\n",
        "#@markdown <br/>\n",
        "token = '' #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "# URL of the CSV file\n",
        "url = 'https://www.weather.gov/source/aprfc/TotalARI_Extract.csv'\n",
        "\n",
        "# Send a GET request to fetch the CSV file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Specify the local path where you want to save the CSV file\n",
        "    output_file = 'TotalARI_Extract.csv'\n",
        "\n",
        "    # Write the content of the CSV file to the local file\n",
        "    with open(output_file, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "    print(f\"File saved successfully as {output_file}\")\n",
        "else:\n",
        "    print(f\"Failed to download the file. Status code: {response.status_code}...will need to re-create the file from scratch.  Sorry!\")\n",
        "    print(\"This will take approximately 30 minutes. You will only need to run this cell one time\")\n",
        "\n",
        "    interval_dict = {4:'1hr', 6:'3hr', 7:'6hr', 8:'12hr', 9:'24hr', 10:'48hr', 11:'72hr'}\n",
        "    recurrence_dict = {0:'1yr', 1:'2yr', 2:'5yr', 3:'10yr', 4:'25yr', 5:'50yr', 6:'100yr',\n",
        "                      7:'200yr', 8:'500yr', 9:'1000yr'}\n",
        "\n",
        "    rifile = 'TotalARI_Extract.csv'\n",
        "    #\n",
        "    # Replace 'your_api_token' with your actual Synoptic API token\n",
        "    API_TOKEN = 'c6c8a66a96094960aabf1fed7d07ccf0'\n",
        "\n",
        "    # API endpoint for getting station metadata\n",
        "    metaurl = 'https://api.synopticdata.com/v2/stations/metadata'\n",
        "\n",
        "    # Parameters for the API request\n",
        "    params = {\n",
        "        'token': API_TOKEN,\n",
        "        'state': 'AK',  # Alaska\n",
        "        'country': 'US',\n",
        "        'status': 'active'  # To get only active stations\n",
        "    }\n",
        "\n",
        "    # Make the API request\n",
        "    response = requests.get(metaurl, params=params)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        #print(data)\n",
        "        # Check if 'STATION' key exists in the response\n",
        "        if 'STATION' in data:\n",
        "            stations = data['STATION']\n",
        "\n",
        "            # Collect station data into a list of dictionaries\n",
        "            station_data = []\n",
        "            for station in stations:\n",
        "                station_id = station.get('STID', 'N/A')\n",
        "                latitude = station.get('LATITUDE', 'N/A')\n",
        "                longitude = station.get('LONGITUDE', 'N/A')\n",
        "                station_data.append({'Site': station_id, 'Lat': latitude, 'Lon': longitude})\n",
        "\n",
        "            # Convert the list of dictionaries into a pandas DataFrame\n",
        "            df = pd.DataFrame(station_data)\n",
        "        else:\n",
        "            print(\"No stations found in the response.\")\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        print(response.text)\n",
        "\n",
        "    # Define the API URL\n",
        "    atlas_url = \"https://hdsc.nws.noaa.gov/cgi-bin/hdsc/new/cgi_readH5.py\"\n",
        "\n",
        "    # Looping through the sites from the dataframe created above\n",
        "    data_dict = {'FID': [], 'Site': [], 'Lat': [], 'Lon': []}\n",
        "    for key in interval_dict:\n",
        "        for i, ari in enumerate(recurrence_dict):\n",
        "          header = f'F{recurrence_dict[ari]}{interval_dict[key]}ARI'\n",
        "          data_dict.update({header:[]})\n",
        "    for index, row in df.iterrows():\n",
        "      site = row['Site']\n",
        "      #print(site)\n",
        "      #if site == 'PAJN' or site == 'PANC':\n",
        "      # Set the query parameters\n",
        "      lat = row['Lat']\n",
        "      lon = row['Lon']\n",
        "      params = {\n",
        "          'lat': lat,\n",
        "          'lon': lon,\n",
        "          'type': 'pf',\n",
        "          'data': 'depth',\n",
        "          'units': 'english',\n",
        "          'series': 'pds'\n",
        "      }\n",
        "      # Construct the full URL with parameters\n",
        "      full_url = requests.Request('GET', atlas_url, params=params).prepare().url\n",
        "      print(f\"Full URL for {site}: {full_url}\")\n",
        "      # Make the API request\n",
        "      response = requests.get(atlas_url, params=params)\n",
        "\n",
        "      # Check if the request was successful\n",
        "      if response.status_code == 200:\n",
        "          # Split the response text to isolate the variable assignments\n",
        "        lines = response.text.split(';')\n",
        "\n",
        "        # Dictionary to store parsed data\n",
        "        parsed_data = {}\n",
        "\n",
        "        # Parse each variable assignment\n",
        "        for line in lines:\n",
        "            if '=' in line:\n",
        "                key, value = line.split('=', 1)\n",
        "                key = key.strip()\n",
        "                value = value.strip()\n",
        "                try:\n",
        "                    # Safely evaluate the value using ast.literal_eval()\n",
        "                    parsed_data[key] = ast.literal_eval(value)\n",
        "                except (ValueError, SyntaxError):\n",
        "                    # If the value cannot be parsed, keep it as a string\n",
        "                    parsed_data[key] = value\n",
        "\n",
        "        # Access the parsed data, for example:\n",
        "        result = parsed_data.get('result')\n",
        "        print(result)\n",
        "        if result == 'none' or result == 'null':\n",
        "          continue\n",
        "        else:\n",
        "          quantiles = parsed_data.get('quantiles')\n",
        "          #loop through the quantiles\n",
        "          data_dict['FID'].append(index)\n",
        "          data_dict['Site'].append(site)\n",
        "          data_dict['Lat'].append(lat)\n",
        "          data_dict['Lon'].append(lon)\n",
        "          for key in interval_dict:\n",
        "            for i, ari in enumerate(recurrence_dict):\n",
        "              header = f'F{recurrence_dict[ari]}{interval_dict[key]}ARI'\n",
        "              data_dict[header].append(quantiles[key][i])\n",
        "      else:\n",
        "          print(f\"Error: {response.status_code}\")\n",
        "    #print(data_dict)\n",
        "    ari_df = pd.DataFrame(data_dict)\n",
        "    #print(ari_df)\n",
        "    ari_df.to_csv(rifile, index=False)\n",
        "    print(f\"Done with extracting ARI data.  {rifile} can be found in the current working directory.\")"
      ],
      "metadata": {
        "id": "JLdCf63WUlLl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3. Select End Data (Valid Time)**\n",
        "Select the valid date/time you want to view the snapshot on the tool.  Date/time format has to be pretty specific so make sure its formatted correctly. Valid times are always in UTC.  You will also need to enter the log-in credentials for the AGOL account (NWS Juneau is where the tool resides)."
      ],
      "metadata": {
        "id": "E-h6rGeDCvFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import shutil\n",
        "from datetime import datetime, timedelta\n",
        "\"\"\"\n",
        "Created on Tue Jan 26 16:54:08 2021\n",
        "\n",
        "@author: David Levin\n",
        "\"\"\"\n",
        "\n",
        "#@markdown **Enter your valid time in \"YYYYmmddhhmm\" format below**\n",
        "END = '' #@param {type:\"string\"}\n",
        "START = datetime.strptime(END, '%Y%m%d%H%M') - timedelta(hours=24)\n",
        "START = START.strftime('%Y%m%d%H%M')\n",
        "###################### Config for the Precipitation Script ###################\n",
        "\n",
        "###################### MesoWest API Config ###################################\n",
        "# The state we want to pull MesoWest data from\n",
        "STATE = 'ak'\n",
        "\n",
        "# To search for the last two hours of observations; recent=120.\n",
        "RECENT = '180'\n",
        "\n",
        "# pmode (totals, intervals, last), defines the interval mode to calculate precipitation.\n",
        "# If omitted the returned JSON formatting will be significantly different.\n",
        "PMODE = 'last'\n",
        "# pmode=intervals, Returns accumulated precipitation for intervals provided in\n",
        "# the additional interval argument. Valid keywords for interval are hour, day,\n",
        "# week, month, year, or non-zero integer in hours. Integers must be a factor or\n",
        "# multiple of 24 (1,2,3,4,6,8,12,24,48,72,etc). Default value is day if\n",
        "# interval is not provided. Partial intervals at the end of a requested range\n",
        "# are still returned. Note that all keywords or integers provided to interval\n",
        "# will use UTC time zone to define the start and end of each interval.\n",
        "# However, each interval respects the requested start hour such that intervals\n",
        "# can be offset for a local time zone.\n",
        "INTERVALS = ['1', '3', '6', '12', '24', '48', '72']\n",
        "\n",
        "# You will need a MesoWest API account.  It's free and you will receive your own token\n",
        "# for downloading data.  An example is included below.\n",
        "TOKEN = token\n",
        "\n",
        "######################## File Paths ###########################################\n",
        "\n",
        "# Where you want your .csv file to go...change this!\n",
        "# Could also add an upload method and have it upload to the web for better\n",
        "# linkage with Arc Online\n",
        "OBS_PATH = '/nas/nomad/ARITool'\n",
        "\n",
        "# Check if the directory exists\n",
        "print(\"Now setting up directories\")\n",
        "if not os.path.exists(OBS_PATH):\n",
        "    # If it doesn't exist, create it\n",
        "    os.makedirs(OBS_PATH)\n",
        "    print(f\"Directory '{OBS_PATH}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Directory '{OBS_PATH}' already exists.\")\n",
        "\n",
        "\n",
        "\n",
        "# Whatever you want to call your precip file (these should be stored in the OBS_PATH directory)\n",
        "PRECIP_OBS_FILE = 'LatestAKPrecipTEST.csv'\n",
        "# The file name of the merged precip data and bad station data\n",
        "FINAL_PRECIP_FILE = 'LatestAKPrecip_BadStations.csv'\n",
        "# The list of your static ARI .csv files which should be in the same\n",
        "# directory as your other data (OBS_PATH)\n",
        "ARI_FILE = 'TotalARI_Extract.csv'\n",
        "# This is the master file which will eventually overwrite the hosted feature layer\n",
        "# on the ArcGis Online server.\n",
        "FINAL_OUTPUT_FILE = 'LatestPrecip_ARITotal_Retrospective_TEST.csv'\n",
        "# Where you would like your log file to be placed\n",
        "LOG_PATH = '/nas/nomad/ARITool'\n",
        "# Name of your log file\n",
        "LOG_FILE = 'meso_west.log'\n",
        "\n",
        "try:\n",
        "  print(\"Now moving some files around to get started...\")\n",
        "  # Source file path (assuming it's in the current working directory)\n",
        "  source_file = ARI_FILE\n",
        "\n",
        "  # Destination directory path\n",
        "  destination_dir = '/nas/nomad/ARITool'\n",
        "\n",
        "  # Destination file path\n",
        "  destination_file = os.path.join(destination_dir, source_file)\n",
        "\n",
        "  # Move the file\n",
        "  shutil.move(source_file, destination_file)\n",
        "\n",
        "  print(f\"File moved successfully to {destination_file}\")\n",
        "except FileNotFoundError:\n",
        "  print(f\"Source file '{source_file}' not found in working directory.\")\n",
        "  print(f\"Will check {OBS_PATH}\")\n",
        "  if os.path.exists(os.path.join(OBS_PATH, ARI_FILE)):\n",
        "    print(f\"{ARI_FILE}' already exists in {OBS_PATH}\")\n",
        "  else:\n",
        "    raise FileNotFoundError(f\"{ARI_FILE} not found in {OBS_PATH}.  Run cell 2 one more time!\")\n",
        "\n",
        "\n",
        "###################### Output Headers & Formating #############################\n",
        "\n",
        "# initializing our output for the .csv file we will generate\n",
        "# Just make sure the headers for the variables are in the same order as the\n",
        "# INTERVALS list above\n",
        "PRECIP_OUTPUT = 'FID,Site,Lat,Lon,DateTime,1hr_Precip,3hr_Precip,6hr_Precip,'\n",
        "PRECIP_OUTPUT += '12hr_Precip,24hr_Precip,48hr_Precip,72hr_Precip\\n'\n",
        "\n",
        "# A list of redundant columns after merging the ARI and MesoWest dataframes\n",
        "# Probably won't have to change this\n",
        "BAD_COLUMNS = ['FID_y', 'Lat_y', 'Lon_y']\n",
        "# for dropping NaN values by column name\n",
        "ARI_HOURS = ['72', '48', '24', '12', '6', '3', '1']\n",
        "\n",
        "# A list of the return intevals you would like to be calculated\n",
        "ARI_LIST = ['1000', '500', '200', '100', '50', '25', '10', '5', '2', '1']\n",
        "\n",
        "#################### ArcGis Login & URLs #####################################\n",
        "#@markdown Login credentials for AGOL.  Make sure to use your office\n",
        "#@markdown Enterprise account login info so that you have full permissions\n",
        "AGOL_USER = '' #@param {type:\"string\"}\n",
        "\n",
        "AGOL_PASSWORD = '' #@param {type:\"string\"}\n",
        "# Url for the hosted feature layer which will be overwritten.  You can grab\n",
        "# this by copying the url after opening the hosted feature layer in the NOAA\n",
        "# Enterprise Account for your office under \"Content\"\n",
        "# Note that this hosted layer has to actually exist for this tool to work.\n",
        "# You will need to manually upload the .csv file the first time. After that, this\n",
        "# should be automated as you will have a url generated after the first upload.\n",
        "AGOL_URL = 'https://services2.arcgis.com/C8EMgrsFcRFL6LrL/arcgis/rest/services/'\n",
        "AGOL_URL += 'LatestPrecip_ARITotal_Retrospective_TEST/FeatureServer'\n",
        "AGOL_NAME = 'LatestPrecip_ARITotal_Retrospective_TEST'\n",
        "#https://services2.arcgis.com/C8EMgrsFcRFL6LrL/arcgis/rest/services/LatestPrecip_ARITotal_Retrospective_TEST/FeatureServer\n",
        "\n",
        "###################### Config for Flagging Bad Stations ######################\n",
        "# URL for the google sheet RFC publishes as a web service\n",
        "STN_URL = 'https://script.googleusercontent.com/macros/echo?user_content_key='\n",
        "STN_URL += '5J_H9sc27K5WgpKbXmbm_zG9LirVMIUFRIggn7LFHkkCbDZU2csoUZH5mTS2uXmfTiv'\n",
        "STN_URL += 'ywganm1M-nO4EO14d-FxMMk8D-Ep5m5_BxDlH2jW0nuo2oDemN9CCS2h10ox_1xSnc'\n",
        "STN_URL += 'GQajx_ryfhECjZEnG-eZm-jLUSFG87Bnj5J5AjenQfNssDMPC4AmEKfQvHjFVxZhIML'\n",
        "STN_URL += 'mTS8mzDTCqnGIg&lib=MXTfKjftoipE0WwmBFaK8ZA0VGsytdp8v'\n",
        "\n",
        "# Columns from the APRFC spreadsheet you want to keep\n",
        "# I chose the \"Status\", \"Status as of\", and \"Notes\" columns\n",
        "COL_INDEX = ['Status', 'Status as of', 'Notes', 'Bad_Obs', 'Good_Obs']\n",
        "# File to which you are writing your text data from the url above\n",
        "BAD_STN_FILE = 'Bad_Stations.txt'\n",
        "# where your parsed data gets placed from the data in the text file above\n",
        "BAD_STN_SHEET = 'APRFC_Bad_Stations.csv'\n",
        "\n",
        "########################## Methods ##############################################\n",
        "\n",
        "def download_precip(start, end, state, recent, mode, ints, token):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    state : The state abbreviation (lower case) for\n",
        "        which you are requesting data.\n",
        "    recent : Number of minutes to look back for new obs from the most recent\n",
        "        time. 180 would be looking back 3 hrs for instance.\n",
        "    mode : There are two modes--intervals and last.  Intervals returns precip\n",
        "        for a requested time interval.  Last returns precip based on the latest\n",
        "        time and you can use 'accum_intervals' to specify a list of intervals\n",
        "        based on this time.  Last is preferred and this is how the method is\n",
        "        set up.\n",
        "    ints : A list of the time intervals for which to pull precip data in hours\n",
        "        (i.e. 1,3,6,12,24)\n",
        "    token : Your MesoWest API\n",
        "        token.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    data : A self describing JSON\n",
        "        object from the MesoWest API.\n",
        "\n",
        "    \"\"\"\n",
        "    n = 0\n",
        "    # Initializing our url\n",
        "    #url = 'https://api.synopticdata.com/v2/stations/precip?'\n",
        "    #url += 'state='+state+'&pmode='+mode+'&recent='+recent+'&accum_hours='\n",
        "    url = 'https://api.synopticdata.com/v2/stations/precip?'\n",
        "    url+='state='+state+'&pmode='+mode+'&start='+start+'&end='+end+'&accum_hours='\n",
        "    # To be tacked on after we add our vars\n",
        "    endurl = '&units=english&output=json&token='+token\n",
        "    # adding our vars\n",
        "    while n < len(ints):\n",
        "        if n != len(ints)-1:\n",
        "            url += ints[n]+','\n",
        "        else:\n",
        "            url += ints[n]\n",
        "        n = n+1\n",
        "    # now adding the end of the url\n",
        "    url = url+endurl\n",
        "    # now requesting the data\n",
        "    page = urllib.request.urlopen(url)\n",
        "    data = page.read()\n",
        "\n",
        "    return data\n",
        "\n",
        "def parse_json(data):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : A self describing\n",
        "        JSON object from MesoWest API\n",
        "    Returns\n",
        "    -------\n",
        "    json_dict : A python dictionary\n",
        "        created from the JSON object.\n",
        "    \"\"\"\n",
        "    # Converting from json to python dictionary\n",
        "    json_dict = json.loads(data)\n",
        "    return json_dict\n",
        "\n",
        "def parse_precip(precip_output, json_dict):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    precip_output : An output string of headers for your precip data which is\n",
        "    pulled from the config file\n",
        "    json_dict : A python dictionary created from a MesoWest JSON query\n",
        "    Returns\n",
        "    -------\n",
        "    precip_output : The original output string with all the data organized and\n",
        "                    added into a comma separated format\n",
        "    \"\"\"\n",
        "    for i in range(0, len(json_dict['STATION'])):\n",
        "        # now pulling the data out of the massive dictionary by looping through the stations\n",
        "        ob = json_dict['STATION'][i]\n",
        "        # ArcGIS nees an FID field to plot so making one up with i\n",
        "        FID = str(i)\n",
        "        site = ob['STID']\n",
        "        #print(site)\n",
        "        lat = ob['LATITUDE']\n",
        "        lon = ob['LONGITUDE']\n",
        "        datetime = ob['OBSERVATIONS']['precipitation'][0]['last_report']\n",
        "        pdata = ob['OBSERVATIONS']['precipitation']\n",
        "        # datetime is the last entry in the \"PERIOD_OF_RECORD\" dictionary\n",
        "        #datetime = ob['PERIOD_OF_RECORD']['end']\n",
        "        precip_output += FID+','+site+','+lat+','+lon+','+datetime+','\n",
        "        # Checking to make sure our precip intervals are there, if no precip\n",
        "        # is reported those intervals are blank in the json data\n",
        "        # if all the intervals are there the length of the list will be 7\n",
        "        # else we need to check for missing intervals\n",
        "        if len(pdata) == 7:\n",
        "            for count, value in enumerate(pdata):\n",
        "                if count != 6:\n",
        "                    # Getting rid of traces that show up as 0.001\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+','\n",
        "                    else:\n",
        "                        precip_output += '0.00,'\n",
        "                else:\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+'\\n'\n",
        "                    else:\n",
        "                        precip_output += '0.00\\n'\n",
        "        elif len(pdata) == 6:\n",
        "            precip_output += ','\n",
        "            for count, value in enumerate(pdata):\n",
        "                if count != 5:\n",
        "                    # Getting rid of traces that show up as 0.001\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+','\n",
        "                    else:\n",
        "                        precip_output += '0.00,'\n",
        "                else:\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+'\\n'\n",
        "                    else:\n",
        "                        precip_output += '0.00\\n'\n",
        "        elif len(pdata) == 5:\n",
        "            precip_output += ',,'\n",
        "            for count, value in enumerate(pdata):\n",
        "                if count != 4:\n",
        "                    # Getting rid of traces that show up as 0.001\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+','\n",
        "                    else:\n",
        "                        precip_output += '0.00,'\n",
        "                else:\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+'\\n'\n",
        "                    else:\n",
        "                        precip_output += '0.00\\n'\n",
        "        elif len(pdata) == 4:\n",
        "            precip_output += ',,,'\n",
        "            for count, value in enumerate(pdata):\n",
        "                if count != 3:\n",
        "                    # Getting rid of traces that show up as 0.001\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+','\n",
        "                    else:\n",
        "                        precip_output += '0.00,'\n",
        "                else:\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+'\\n'\n",
        "                    else:\n",
        "                        precip_output += '0.00\\n'\n",
        "        elif len(pdata) == 3:\n",
        "            precip_output += ',,,,'\n",
        "            for count, value in enumerate(pdata):\n",
        "                if count != 2:\n",
        "                    # Getting rid of traces that show up as 0.001\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+','\n",
        "                    else:\n",
        "                        precip_output += '0.00,'\n",
        "                else:\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+'\\n'\n",
        "                    else:\n",
        "                        precip_output += '0.00\\n'\n",
        "        elif len(pdata) == 2:\n",
        "            precip_output += ',,,,,'\n",
        "            for count, value in enumerate(pdata):\n",
        "                if count != 1:\n",
        "                    # Getting rid of traces that show up as 0.001\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+','\n",
        "                    else:\n",
        "                        precip_output += '0.00,'\n",
        "                else:\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+'\\n'\n",
        "                    else:\n",
        "                        precip_output += '0.00\\n'\n",
        "        elif len(pdata) == 1:\n",
        "            precip_output += ',,,,,,'\n",
        "            for count, value in enumerate(pdata):\n",
        "                if value['total'] >= 0.01:\n",
        "                    precip_output += str(round(value['total'], 2))+'\\n'\n",
        "                else:\n",
        "                    precip_output += '0.00\\n'\n",
        "    return precip_output\n",
        "\n",
        "def grab_bad_stnlist(url):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    url : a url from which you wish to scrape data\n",
        "    Returns\n",
        "    -------\n",
        "    data : the data from your urllib.request object\n",
        "\n",
        "    \"\"\"\n",
        "    # now requesting the bad station list from APRFC\n",
        "    page = urllib.request.urlopen(url)\n",
        "    data = page.read()\n",
        "\n",
        "    return data\n",
        "\n",
        "def write_to_file(data, path, fname, binary=True):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : raw data (such as from a web scrape)\n",
        "    path : where you want your data stored\n",
        "    fname : name of your output file\n",
        "    binary : write mode of 'wb' if True and 'w' if False\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    \"\"\"\n",
        "    if binary:\n",
        "        writeflag = 'wb'\n",
        "    else:\n",
        "        writeflag = 'w'\n",
        "    outfile = open(os.path.join(path, fname), writeflag)\n",
        "    outfile.write(data)\n",
        "    outfile.close()\n",
        "\n",
        "def format_data(path, fname):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    path : path to your data\n",
        "    fname : name of your text file\n",
        "    Returns\n",
        "    -------\n",
        "    dict_fm_file : formatted data (python dictionary/list from your text data)\n",
        "\n",
        "    \"\"\"\n",
        "    with open(os.path.join(path, fname)) as newfile:\n",
        "        #dict_fm_file = eval(newfile.read())\n",
        "        dict_fm_file = json.loads(newfile.read())\n",
        "        newfile.close()\n",
        "\n",
        "    return dict_fm_file\n",
        "\n",
        "def build_badstn_dict(station_list):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    station_list : a python list of dictionaries formatted from the APRFC web\n",
        "        service google sheet of bad station data\n",
        "    Returns\n",
        "    -------\n",
        "    stn_dict : reformats the data into a dictionary with the keys being\n",
        "        column headers in the original spreadsheet and the values being a list\n",
        "        containing the column values\n",
        "\n",
        "    \"\"\"\n",
        "    stn_dict = {}\n",
        "    stn_keys = station_list[0].keys()\n",
        "    for key in stn_keys:\n",
        "        values_list = []\n",
        "        for stn in station_list:\n",
        "            values_list.append(stn[key])\n",
        "        stn_dict.update({key:values_list})\n",
        "    return stn_dict\n",
        "\n",
        "def create_badstn_dataframe(df_dict, drop_col, drop_dupes=True):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_dict : a python dictionary from which you want a dataframe\n",
        "    drop_col : A column (string) you wish to search for duplicate values\n",
        "    drop_dupes : if set to True will drop all rows that have duplicate values\n",
        "    in the drop_col variable\n",
        "    Returns\n",
        "    -------\n",
        "    newdf : a pandas dataframe\n",
        "\n",
        "    \"\"\"\n",
        "    newdf = pd.DataFrame(df_dict)\n",
        "    if drop_dupes:\n",
        "        newdf.drop_duplicates(drop_col, inplace=True)\n",
        "        newdf.reset_index(inplace=True)\n",
        "    else:\n",
        "        pass\n",
        "    return newdf\n",
        "\n",
        "def save_badstn_dataframe(df, path, dfname, cols_to_keep, col_to_drop,\n",
        "                          dropcols=True, dropindex=True, keep_cols=True):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : a pandas dataframe you wish to save as a .csv\n",
        "    path : where you want your file saved\n",
        "    dfname : what you want to call your file\n",
        "    cols_to_keep : a list of columns you wish to save (can be an empty list,\n",
        "                                                       just set keep_cols to False)\n",
        "    col_to_drop : A single column (string) you wish to drop from your dataframe.  Can be\n",
        "    empty string.\n",
        "    dropcols : You can choose to drop a single column if true.  Otherwise\n",
        "    better just to use cols_to_keep with keep_cols = True\n",
        "    dropindex : Deleting the index column if desired (True)\n",
        "    keep_cols : keeps the columns passed in the cols_to_keep list if True\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    \"\"\"\n",
        "    #print(\"Dropping this column %s\" %(col_to_drop))\n",
        "    #print(\"Keeping these columns %s\" %(cols_to_keep))\n",
        "    if dropcols:\n",
        "        final_df = df.drop([col_to_drop], axis=1)\n",
        "    else:\n",
        "        final_df = df\n",
        "    if dropindex and keep_cols:\n",
        "        final_df = final_df.reindex(columns=cols_to_keep)\n",
        "        final_df.to_csv(os.path.join(path, dfname), columns=cols_to_keep, index=False)\n",
        "    elif dropindex and not keep_cols:\n",
        "        final_df.to_csv(os.path.join(path, dfname), index=False)\n",
        "    elif not dropindex and keep_cols:\n",
        "        final_df = final_df.reindex(columns=cols_to_keep)\n",
        "        final_df.to_csv(os.path.join(path, dfname), columns=cols_to_keep, index=False)\n",
        "    elif not dropindex and not keep_cols:\n",
        "        final_df.to_csv(os.path.join(path, dfname))\n",
        "\n",
        "def merge_badstn_dataframe(df1, df2, path, stn_columns, merge_how, left_merge, right_merge):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df1 : the primary .csv file (keeping all data)\n",
        "    df2 : secondary .csv file (you are merging this data with df1)\n",
        "    stn_columns : a list of columns you wish to keep from the stn_data df (can be empty)\n",
        "    how : How you want to merge (\"left\" is preferred but if you choose \"right\" you will keep\n",
        "                                 all data from df2 instead)\n",
        "    left_on : Column in df1 that you wish to merge on\n",
        "    right_on : Column in df2 that you wish to merge on\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    added_df : A merge of df1 and df2 with dummy columns added for bad and good obs\n",
        "    new_cols : A list of columns you wish to keep in your final df\n",
        "\n",
        "    \"\"\"\n",
        "    first_df = pd.read_csv(os.path.join(path, df1))\n",
        "    second_df = pd.read_csv(os.path.join(path, df2))\n",
        "    cols_to_keep = first_df.columns.tolist()\n",
        "    new_cols = cols_to_keep + stn_columns\n",
        "    combo_df = first_df.merge(second_df, how=merge_how,\n",
        "                              left_on=left_merge, right_on=right_merge)\n",
        "    added_df = add_dummy_cols(combo_df)\n",
        "    return added_df, new_cols\n",
        "\n",
        "def add_dummy_cols(df1):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df1 : A Pandas dataframe of bad station data from APRFC.\n",
        "    Returns\n",
        "    -------\n",
        "    df1 : The same dataframe with columns added for the gauge status\n",
        "    \"\"\"\n",
        "    df1['Bad_Obs'] = np.where(df1['Status'] == 'Bad', 'Bad', '')\n",
        "    df1['Good_Obs'] = np.where(df1['Status'] != 'Bad', 'Good', '')\n",
        "    return df1\n",
        "\n",
        "def merge_csv(path, first, second, cols_to_del, hour):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path : The path for your precip data and your ARI (should be in the same location)\n",
        "    first : An input .csv file containing the ARI data extracted from PFDS\n",
        "    second : An input .csv file containing the latest MesoWest Precip obs\n",
        "    col_to_del : A list of redundant columns you wish to delete after merging\n",
        "            usually lat/lon and FID and can be read from config\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    final_df : A pandas dataframe containing the merged MesoWest precip data\n",
        "            and the ARI data\n",
        "\n",
        "    \"\"\"\n",
        "    df1 = pd.read_csv(os.path.join(path, first))\n",
        "    df2 = pd.read_csv(os.path.join(path, second))\n",
        "    combined_df = df1.merge(df2, how='left', left_on='Site', right_on='Site', suffixes=('', '_y'))\n",
        "    # dropping stations which don't have ARIs\n",
        "    clean_merged_df = combined_df.dropna(subset=['F1000yr'+hour+'hrARI'])\n",
        "    final_df = clean_merged_df.drop(columns=cols_to_del)\n",
        "    return final_df\n",
        "\n",
        "def calc_all_percent_exceedance(path, infile, hours, ri_list):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path : The path for your precip data and your ARI (should be in the same location)\n",
        "    infile : An input .csv file containing the merged MesoWest precip obs and\n",
        "            ARI data from PFDS\n",
        "    hours : A list of the time intervals for the accumulated precip (strings such as: '24','12')\n",
        "    ri_list : A list of the ARI for which you want percent\n",
        "           exceedances (1, 2, 5, 10 year ARI etc)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    clean_exceedance_df : A pandas dataframe containing the merged MesoWest precip data\n",
        "            and the ARI data with percent exceedances calculated and the RIs\n",
        "            dropped to clean up the file a bit\n",
        "\n",
        "    \"\"\"\n",
        "    exceedance_df = pd.read_csv(os.path.join(path, infile))\n",
        "    # calculating % exceedance at each precip interval, for each RI at that interval\n",
        "    # initializing a list of RI columns we can now drop since we no longer need them\n",
        "    drop_list = []\n",
        "    for hr in hours:\n",
        "        for col in ri_list:\n",
        "            exceedance_df[col+'yr_'+hr+'hr_PercentExceedance'] = \\\n",
        "            round((exceedance_df[hr+'hr_Precip']/exceedance_df['F'+col+'yr'+hr+'hrARI'])*100, 0)\n",
        "            drop_list.append('F'+col+'yr'+hr+'hrARI')\n",
        "    # Now dropping the RI columns as we don't need them any more\n",
        "    clean_exceedance_df = exceedance_df.drop(drop_list, axis=1)\n",
        "    return clean_exceedance_df\n",
        "\n",
        "# Need to pass an hours list\n",
        "def calc_all_ari(path, infile, hours, ri_list):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path : The path for your precip data and your ARI (should be in the same location)\n",
        "    infile : An input .csv file containing the merged MesoWest precip obs and\n",
        "            ARI data from PFDS along with the calculated % exceedance (important!)\n",
        "    hours : A list of the time intervals for the accumulated precip (string such as: '24','12')\n",
        "    ri_list : A list of the ARI for which you want percent\n",
        "           exceedances (1, 2, 5, 10 year ARI etc)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ri_df : A pandas dataframe containing the merged MesoWest precip data\n",
        "            and the ARI data with the ARI calculated for each ob\n",
        "\n",
        "    \"\"\"\n",
        "    # opening our file\n",
        "    ri_df = pd.read_csv(os.path.join(path, infile))\n",
        "    # Looping through the precip accumulation intervals\n",
        "    for hr in hours:\n",
        "        # Initiating our new columns\n",
        "        col_names = []\n",
        "        # Now naming our columns from the list of ARIs\n",
        "        for col in ri_list:\n",
        "            col_names.append(col+'yr_'+hr+'hr_PercentExceedance')\n",
        "        #Initializing our list of conditions to check our data against\n",
        "        conditions = []\n",
        "        # Initializing the list of values we want to place in our RI column\n",
        "        # when certain conditions are met\n",
        "        output = []\n",
        "        # We want to check each percent exceedance column for values > 100%\n",
        "        for value in col_names:\n",
        "            conditions.append(ri_df[value] >= 100)\n",
        "        # We want to fill our RI column with the return interval that corresponds\n",
        "        # to the met condition\n",
        "        for ari in ri_list:\n",
        "            output.append(float(ari))\n",
        "        #print('Output is: ')\n",
        "        #print(output)\n",
        "        # Now for amounts that do not exceed any RI, we have a seperate condition\n",
        "        conditions.append(ri_df[col_names[len(col_names)-1]] < 100)\n",
        "        # For this, we want to append the actual percent exceedance/100\n",
        "        output.append(round(ri_df[col_names[9]].div(100), 2))\n",
        "        # Now we add a new column to our dataframe with the calculated output\n",
        "        # values generated from our conditions\n",
        "        ri_df[hr+'hr_RI'] = np.select(conditions, output)\n",
        "    # calculating which precip accumulation interval has the highest RI\n",
        "    ri_df['Max_RI'] = ri_df[['72hr_RI','48hr_RI','24hr_RI','12hr_RI','6hr_RI','3hr_RI','1hr_RI']].max(axis=1)\n",
        "    return ri_df\n",
        "\n",
        "def replace_gis_file(user, pw, path, myfile, agol_name):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    user : Your AGOL username (string)\n",
        "    pw : Your AGOL password (string)\n",
        "    path : The file path where your .csv file is stored (string)\n",
        "    myfile : The filename of your .csv file (string)\n",
        "    agol_name : The name of your hosted feature layer (should be the filename\n",
        "                                                       without the extention\n",
        "                                                       i.e. 'AK_Obs' instead\n",
        "                                                       of 'AK_Obs.csv')\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    # GIS logs pretty much everything for you so no need to call logger here\n",
        "    # Now logging into Arc Online to update the hosted layer\n",
        "    gis = GIS('https://noaa.maps.arcgis.com/home', username=user, password=pw)\n",
        "\n",
        "    try:\n",
        "        data_file   = os.path.join(path, myfile)\n",
        "        #### Delete prior to reposting ####\n",
        "        item_types = [\"CSV\", \"Feature Layer Collection\"]\n",
        "        name_list = [agol_name]\n",
        "        for current_item_type in item_types:\n",
        "            for file_name in name_list:\n",
        "                search_result = gis.content.search(query=file_name, item_type=current_item_type)\n",
        "                if len(search_result) > 0:\n",
        "                    for item in search_result:\n",
        "                        item.delete()\n",
        "                        print(\"Deleted existing \" + current_item_type + \": \", item)\n",
        "\n",
        "        #### Replace file ####\n",
        "        csv_item   = gis.content.add({}, data_file)\n",
        "\n",
        "        #### Re-Publish File ####\n",
        "        primary_feature_layer   = csv_item.publish()\n",
        "\n",
        "        print (primary_feature_layer.url)\n",
        "\n",
        "    except IOError:\n",
        "        print(\"GIS error\")\n",
        "\n",
        "def do_some_gis(user, pw, path, myurl, myfile):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    user : Your AGOL username (string)\n",
        "    pw : Your AGOL password (string)\n",
        "    path : The file path where your .csv file is stored (string)\n",
        "    myurl : The URL of your hosted feature layer service from AGOL (string)\n",
        "    myfile : The filename of your .csv file (string)\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    # GIS logs pretty much everything for you so no need to call logger here\n",
        "    # Now logging into Arc Online to update the hosted layer\n",
        "    login = GIS('https://noaa.maps.arcgis.com/home', username=user, password=pw)\n",
        "    # Accessing my feature layer collection by using the url generated from\n",
        "    # within AGOL.\n",
        "    my_content = myurl\n",
        "    # Use the url to create a FeatureLayerCollection object which you will overwrite with the\n",
        "    # new data.\n",
        "    akobs = FeatureLayerCollection(my_content, login)\n",
        "    # Now overwriting the old .csv file with the new one.\n",
        "    # Make sure your obs_path and filenames match\n",
        "    akobs.manager.overwrite(os.path.join(path, myfile))\n",
        "\n",
        "def create_hex_colors(ripath, rifile):\n",
        "    ri_columns = ['1hr_RI','3hr_RI','6hr_RI','12hr_RI','24hr_RI','48hr_RI','72hr_RI','Max_RI']\n",
        "    new_columns = ['1hrRI_Color','3hrRI_Color','6hrRI_Color','12hrRI_Color','24hrRI_Color',\n",
        "                   '48hrRI_Color','72hrRI_Color','MaxRI_Color']\n",
        "    ridf = pd.read_csv(os.path.join(ripath, rifile))\n",
        "    for count, col in enumerate(new_columns):\n",
        "        print(ri_columns[count])\n",
        "        conditions = [ridf[ri_columns[count]] <= 1, ridf[ri_columns[count]] == 2, ridf[ri_columns[count]] == 5,\n",
        "                      ridf[ri_columns[count]] == 10, ridf[ri_columns[count]] == 25, ridf[ri_columns[count]] == 50,\n",
        "                      ridf[ri_columns[count]] == 100, ridf[ri_columns[count]] == 200, ridf[ri_columns[count]] == 500,\n",
        "                      ridf[ri_columns[count]] == 1000, ridf[ri_columns[count]] == 2000]\n",
        "        values = ['#55FF00','#FFFF00','#FFAA00','#FF0000','#FF99FF','#FF00FF','#660066','#FFFFFF',\n",
        "                  '#FFFFFF', '#FFFFFF', '#FFFFFF']\n",
        "        ridf[col] = np.select(conditions, values)\n",
        "    return ridf\n",
        "######################## Main Code ##############################################\n",
        "\n",
        "def execute(START, END):\n",
        "    # Setting up our logging\n",
        "    # using the default root logger\n",
        "    for handler in logging.root.handlers[:]:\n",
        "        logging.root.removeHandler(handler)\n",
        "    # Configuring our log\n",
        "    logging.basicConfig(filename=os.path.join(LOG_PATH, LOG_FILE), filemode='w',\n",
        "                        format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S',\n",
        "                        level=logging.DEBUG)\n",
        "    # Grabbing our logger\n",
        "    logger = logging.getLogger('')\n",
        "\n",
        "    # grabbing our data\n",
        "    precip_data = download_precip(START, END, STATE, RECENT, PMODE, INTERVALS, TOKEN)\n",
        "    logger.info('Grabbed JSON object from MesoWest')\n",
        "\n",
        "    # parsing the raw JSON into a python-readable dictionary\n",
        "    parsed_precip = parse_json(precip_data)\n",
        "    logger.info('Now checking to see if we have a valid query')\n",
        "    # Checking to see if we made a valid request from the MesoWest API for precip\n",
        "    if parsed_precip['SUMMARY']['NUMBER_OF_OBJECTS'] != 0:\n",
        "        logger.info('Found valid MesoWest API query')\n",
        "    else:\n",
        "        # printing the error message from the JSON object to the log file\n",
        "        logger.warning('Invalid JSON request! %s', parsed_precip['SUMMARY']['RESPONSE_MESSAGE'])\n",
        "        #sys.exit()\n",
        "    logger.info('Now looping through the stations')\n",
        "    # Creating the final precip output string\n",
        "    final_output = parse_precip(PRECIP_OUTPUT, parsed_precip)\n",
        "    # Writing the output to the PRECIP_OBS_FILE\n",
        "    f = open(os.path.join(OBS_PATH, PRECIP_OBS_FILE), 'w')\n",
        "    f.write(final_output)\n",
        "    f.close()\n",
        "    logger.info('Final output now written to %s', PRECIP_OBS_FILE)\n",
        "    # Now grabbing bad stations from APRFC and appending to the precip file\n",
        "    logger.info('Now grabbing the bad station list from APRFC')\n",
        "    write_to_file(grab_bad_stnlist(STN_URL), OBS_PATH, BAD_STN_FILE)\n",
        "    # Formatting the raw text into a python list\n",
        "    stn_data = format_data(OBS_PATH, BAD_STN_FILE)\n",
        "    # reformating the list into a dictionary for ease of transition into pandas\n",
        "    station_dictionary = build_badstn_dict(stn_data)\n",
        "    # Creating our dataframe from the bad station data\n",
        "    station_dataframe = create_badstn_dataframe(station_dictionary, 'Location ID')\n",
        "    # Saving the file\n",
        "    logger.info('Now saving the bad station data as %s', BAD_STN_SHEET)\n",
        "    save_badstn_dataframe(station_dataframe, OBS_PATH, BAD_STN_SHEET,\n",
        "                          station_dataframe.columns, 'Row', keep_cols=False)\n",
        "    # Now merging it with the precip file\n",
        "    logger.info('Now merging precip data with the bad station list from APRFC')\n",
        "    merged_df, columns = merge_badstn_dataframe(PRECIP_OBS_FILE, BAD_STN_SHEET,\n",
        "                                                OBS_PATH, COL_INDEX, 'left', 'Site', 'Location ID')\n",
        "    # Now saving the precip file with the bad station data appended\n",
        "    save_badstn_dataframe(merged_df, OBS_PATH, FINAL_PRECIP_FILE, columns, '', dropcols=False)\n",
        "    logger.info('All done updating %s with bad stations!', FINAL_PRECIP_FILE)\n",
        "    print(\"All Done Updating Bad Stations!\")\n",
        "    # Merging the precip output with the ARI file\n",
        "    logger.info('Now merging precip data with the ARIs')\n",
        "    # # No longer a loop through multiple ARI files\n",
        "    # Added index = False on the to_csv call to remove the index column\n",
        "    precip_df = merge_csv(OBS_PATH, FINAL_PRECIP_FILE, ARI_FILE, BAD_COLUMNS, '24')\n",
        "    precip_df.to_csv(os.path.join(OBS_PATH, FINAL_OUTPUT_FILE), index=False)\n",
        "    # Calculating the percent exceedances for each precip interval at each station\n",
        "    ex_df = calc_all_percent_exceedance(OBS_PATH, FINAL_OUTPUT_FILE, ARI_HOURS, ARI_LIST)\n",
        "    ex_df.to_csv(os.path.join(OBS_PATH, FINAL_OUTPUT_FILE), index=False)\n",
        "    # Calculating the return intervals for each precip interval at each station\n",
        "    ari_df = calc_all_ari(OBS_PATH, FINAL_OUTPUT_FILE, ARI_HOURS, ARI_LIST)\n",
        "    ari_df.to_csv(os.path.join(OBS_PATH, FINAL_OUTPUT_FILE), index=False)\n",
        "    ari_color_df = create_hex_colors(OBS_PATH, FINAL_OUTPUT_FILE)\n",
        "    ari_color_df.to_csv(os.path.join(OBS_PATH, FINAL_OUTPUT_FILE), index=False)\n",
        "    logger.info('All done updating %s with final output!', FINAL_OUTPUT_FILE)\n",
        "    print(\"All done updating \"+FINAL_OUTPUT_FILE+\" with final output\")\n",
        "    # This will upload the new .csv of precip/ari data to AGOL and overwrite the\n",
        "    # hosted feature layer.  If you for some reason need to add a new column to the\n",
        "    # .csv you will need to comment this line out and run the 'replace_gis_file'\n",
        "    # function below\n",
        "    do_some_gis(AGOL_USER, AGOL_PASSWORD, OBS_PATH, AGOL_URL, FINAL_OUTPUT_FILE)\n",
        "    logger.info('All done updating %s on the AGOL server', AGOL_URL)\n",
        "    print('All done updating %s on the AGOL server' %(AGOL_URL))\n",
        "    # Use the below line ONLY if you need to add a column to the final output .csv file\n",
        "    # This will delete the hosted feature layer and replace it with a new one\n",
        "    # You will need to comment out the 'do_some_gis' line above\n",
        "    #replace_gis_file(AGOL_USER, AGOL_PASSWORD, OBS_PATH, FINAL_OUTPUT_FILE, AGOL_NAME)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    try:\n",
        "        test_start = datetime.strptime(START, '%Y%m%d%H%M')\n",
        "        test_end = datetime.strptime(END, '%Y%m%d%H%M')\n",
        "        execute(START, END)\n",
        "    except ValueError:\n",
        "        test_start = 0\n",
        "        test_end = 0\n",
        "        print('Wrong datetime format! Must be YYYYmmddHHMM. Ex: 202012021800')\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1HrZBivODL0d"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "k6DRBRAXCyNh",
        "pNRHFuNmC-2K"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:33) \n[Clang 13.0.1 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "d9efab89797b4f7e4129f7fe7c375038c6a3f1b6c83da7efdea02c4da588d5be"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}