{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/david-levin11/Verification_Notebooks/blob/main/APRFC_Recurrence_Interval_Retrospective_Tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iYyzL_9EAm7"
      },
      "source": [
        "# **Alaska Region Precipitation & Recurrence Interval Retrospective Tool**\n",
        "<br/>\n",
        "Description--This tool will update the ArcGIS Online retrospective version of the Alaska Region Precipitation & Recurrence Interval tool to look at past cases/events.\n",
        "\n",
        "**UPDATE 09/27/2024: For the APRFC have added the ability to quickly add sites to the tool which will update both the retrospective AND the real time tool.  For this feature just run steps 1 and 2**\n",
        "\n",
        "- David Levin, Arctic Testbed & Proving Ground, Anchorage Alaska"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t56SexSlUELy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNRHFuNmC-2K"
      },
      "source": [
        "##**1 - Install and Import Packages**\n",
        "This will take about a minute to run.  **Only run this cell one time.  Then you can generate as many snapshots as you need using the below steps.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJaogqYCNB-Q",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "!pip install arcgis\n",
        "import os\n",
        "import urllib\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import urllib3\n",
        "import ast\n",
        "import logging\n",
        "import shutil\n",
        "from datetime import datetime, timedelta\n",
        "from arcgis.gis import GIS\n",
        "from arcgis.features import FeatureLayerCollection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2. Update Point ARI Data**\n",
        "If the ARI file exists on CMS this will take milliseconds.  If it does not exist the file will have to be created on the fly.  If this is the case the average run time is around 35 minutes.  **You only need to run this cell once per session.  Then you can generate as many snapshots as you need by repeating step 3 below**\n",
        "\n"
      ],
      "metadata": {
        "id": "-cr99dD1FLeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@markdown **Enter your Synoptic API token below**\n",
        "#@markdown <br/>\n",
        "token = 'c6c8a66a96094960aabf1fed7d07ccf0' #@param {type:\"string\"}\n",
        "#@markdown **Update the tool with new sites?  If so, enter a comma seperated list of site ids (or a single site id) below (EX: c6840,pmma2)**\n",
        "add_new_sites = True #@param {type:\"boolean\"}\n",
        "new_sites = 'wera2' #@param {type:\"string\"}\n",
        "if add_new_sites:\n",
        "  new_sites = [x.lower() for x in new_sites.split(',')]\n",
        "\n",
        "interval_dict = {4:'1hr', 6:'3hr', 7:'6hr', 8:'12hr', 9:'24hr', 10:'48hr', 11:'72hr'}\n",
        "recurrence_dict = {0:'1yr', 1:'2yr', 2:'5yr', 3:'10yr', 4:'25yr', 5:'50yr', 6:'100yr',\n",
        "                      7:'200yr', 8:'500yr', 9:'1000yr'}\n",
        "\n",
        "# disabling url request warnings...seems to pop up now with the latest\n",
        "# version of colab\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "def get_metadata(site, token):\n",
        "  metadata_api = f\"https://api.synopticdata.com/v2/stations/metadata?\"\n",
        "  meta_api_args = {\"token\":token,\"stid\":site}\n",
        "  req = requests.get(metadata_api, params=meta_api_args)\n",
        "  # Check if the request was successful (status code 200)\n",
        "  if req.status_code == 200:\n",
        "    metadata = req.json()\n",
        "  else:\n",
        "    print(f\"Error: {req.status_code}\")\n",
        "    print(req.text)\n",
        "  try:\n",
        "    lat = metadata['STATION'][0]['LATITUDE']\n",
        "    lon = metadata['STATION'][0]['LONGITUDE']\n",
        "    site_df = pd.DataFrame({'Site':site.lower(), 'Lat':round(float(lat),2), 'Lon':round(float(lon),2)}, index=[0])\n",
        "    return site_df\n",
        "  except Exception:\n",
        "    metadata['SUMMARY']['RESPONSE_MESSAGE']\n",
        "    return metadata['SUMMARY']['RESPONSE_MESSAGE']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# URL of the CSV file\n",
        "url = 'https://www.weather.gov/source/aprfc/TotalARI_Extract_Version3.csv'\n",
        "\n",
        "# Send a GET request to fetch the CSV file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Specify the local path where you want to save the CSV file\n",
        "    output_file = 'TotalARI_Extract_Version3.csv'\n",
        "\n",
        "    # Write the content of the CSV file to the local file\n",
        "    with open(output_file, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "    print(f\"File saved successfully as {output_file}\")\n",
        "else:\n",
        "    print(f\"Failed to download the file. Status code: {response.status_code}...will need to re-create the file from scratch.  Sorry!\")\n",
        "    print(\"This will take approximately 30 minutes. You will only need to run this cell one time\")\n",
        "\n",
        "\n",
        "    rifile = 'TotalARI_Extract_Version3.csv'\n",
        "    #\n",
        "    # Replace 'your_api_token' with your actual Synoptic API token\n",
        "    API_TOKEN = 'c6c8a66a96094960aabf1fed7d07ccf0'\n",
        "\n",
        "    # API endpoint for getting station metadata\n",
        "    metaurl = 'https://api.synopticdata.com/v2/stations/metadata'\n",
        "\n",
        "    # Parameters for the API request\n",
        "    params = {\n",
        "        'token': API_TOKEN,\n",
        "        'state': 'AK',  # Alaska\n",
        "        'country': 'US',\n",
        "        'status': 'active'  # To get only active stations\n",
        "    }\n",
        "\n",
        "    # Make the API request\n",
        "    response = requests.get(metaurl, params=params)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        #print(data)\n",
        "        # Check if 'STATION' key exists in the response\n",
        "        if 'STATION' in data:\n",
        "            stations = data['STATION']\n",
        "\n",
        "            # Collect station data into a list of dictionaries\n",
        "            station_data = []\n",
        "            for station in stations:\n",
        "                station_id = station.get('STID', 'N/A')\n",
        "                latitude = station.get('LATITUDE', 'N/A')\n",
        "                longitude = station.get('LONGITUDE', 'N/A')\n",
        "                station_data.append({'Site': station_id, 'Lat': latitude, 'Lon': longitude})\n",
        "\n",
        "            # Convert the list of dictionaries into a pandas DataFrame\n",
        "            df = pd.DataFrame(station_data)\n",
        "        else:\n",
        "            print(\"No stations found in the response.\")\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        print(response.text)\n",
        "\n",
        "    # Define the API URL\n",
        "    atlas_url = \"https://hdsc.nws.noaa.gov/cgi-bin/hdsc/new/cgi_readH5.py\"\n",
        "\n",
        "    # Looping through the sites from the dataframe created above\n",
        "    data_dict = {'FID': [], 'Site': [], 'Lat': [], 'Lon': []}\n",
        "    for key in interval_dict:\n",
        "        for i, ari in enumerate(recurrence_dict):\n",
        "          header = f'F{recurrence_dict[ari]}{interval_dict[key]}ARI'\n",
        "          data_dict.update({header:[]})\n",
        "    for index, row in df.iterrows():\n",
        "      site = row['Site']\n",
        "      #print(site)\n",
        "      #if site == 'PAJN' or site == 'PANC':\n",
        "      # Set the query parameters\n",
        "      lat = row['Lat']\n",
        "      lon = row['Lon']\n",
        "      params = {\n",
        "          'lat': lat,\n",
        "          'lon': lon,\n",
        "          'type': 'pf',\n",
        "          'data': 'depth',\n",
        "          'units': 'english',\n",
        "          'series': 'pds'\n",
        "      }\n",
        "      # Construct the full URL with parameters\n",
        "      full_url = requests.Request('GET', atlas_url, params=params).prepare().url\n",
        "      print(f\"Full URL for {site}: {full_url}\")\n",
        "      # Make the API request\n",
        "      response = requests.get(atlas_url, params=params)\n",
        "\n",
        "      # Check if the request was successful\n",
        "      if response.status_code == 200:\n",
        "          # Split the response text to isolate the variable assignments\n",
        "        lines = response.text.split(';')\n",
        "\n",
        "        # Dictionary to store parsed data\n",
        "        parsed_data = {}\n",
        "\n",
        "        # Parse each variable assignment\n",
        "        for line in lines:\n",
        "            if '=' in line:\n",
        "                key, value = line.split('=', 1)\n",
        "                key = key.strip()\n",
        "                value = value.strip()\n",
        "                try:\n",
        "                    # Safely evaluate the value using ast.literal_eval()\n",
        "                    parsed_data[key] = ast.literal_eval(value)\n",
        "                except (ValueError, SyntaxError):\n",
        "                    # If the value cannot be parsed, keep it as a string\n",
        "                    parsed_data[key] = value\n",
        "\n",
        "        # Access the parsed data, for example:\n",
        "        result = parsed_data.get('result')\n",
        "        print(result)\n",
        "        if result == 'none' or result == 'null':\n",
        "          continue\n",
        "        else:\n",
        "          quantiles = parsed_data.get('quantiles')\n",
        "          #loop through the quantiles\n",
        "          data_dict['FID'].append(index)\n",
        "          data_dict['Site'].append(site)\n",
        "          data_dict['Lat'].append(lat)\n",
        "          data_dict['Lon'].append(lon)\n",
        "          for key in interval_dict:\n",
        "            for i, ari in enumerate(recurrence_dict):\n",
        "              header = f'F{recurrence_dict[ari]}{interval_dict[key]}ARI'\n",
        "              data_dict[header].append(quantiles[key][i])\n",
        "      else:\n",
        "          print(f\"Error: {response.status_code}\")\n",
        "    #print(data_dict)\n",
        "    ari_df = pd.DataFrame(data_dict)\n",
        "    #print(ari_df)\n",
        "    ari_df.to_csv(rifile, index=False)\n",
        "    print(f\"Done with extracting ARI data.  {rifile} can be found in the current working directory.\")\n",
        "\n",
        "# after data download or creation, can add any new sites\n",
        "if add_new_sites:\n",
        "  print(f\"Now adding new sites to {output_file if os.path.exists(output_file) else rifile}\")\n",
        "  for site in new_sites:\n",
        "    # opening our ARI file\n",
        "    ari_output = output_file if os.path.exists(output_file) else rifile\n",
        "    old_ari_df = pd.read_csv(ari_output)\n",
        "    site_list = old_ari_df['Site'].tolist()\n",
        "    new_fid = old_ari_df['FID'].max() + 1\n",
        "\n",
        "    if site.upper() in site_list:\n",
        "      print(f\"Site {site} already exists in {ari_output}\")\n",
        "      print(\"Skipping for now...\")\n",
        "      continue\n",
        "    else:\n",
        "      print(f\"Site {site} does not exist in {ari_output}\")\n",
        "      print(f\"Getting metadata for site {site}\")\n",
        "      try:\n",
        "        new_site_df = get_metadata(site, token)\n",
        "        print(new_site_df)\n",
        "      except Exception:\n",
        "        print(f\"Error getting metadata for site {site}.  See response message below\")\n",
        "        print(new_site_df)\n",
        "        continue\n",
        "      # Define the API URL\n",
        "      atlas_url = \"https://hdsc.nws.noaa.gov/cgi-bin/hdsc/new/cgi_readH5.py\"\n",
        "      try:\n",
        "        # Looping through the sites from the dataframe created above\n",
        "        data_dict = {'FID': [], 'Site': [], 'Lat': [], 'Lon': []}\n",
        "        for key in interval_dict:\n",
        "            for i, ari in enumerate(recurrence_dict):\n",
        "              header = f'F{recurrence_dict[ari]}{interval_dict[key]}ARI'\n",
        "              data_dict.update({header:[]})\n",
        "        for index, row in new_site_df.iterrows():\n",
        "          site = row['Site']\n",
        "          #print(site)\n",
        "          #if site == 'PAJN' or site == 'PANC':\n",
        "          # Set the query parameters\n",
        "          lat = row['Lat']\n",
        "          lon = row['Lon']\n",
        "          params = {\n",
        "              'lat': lat,\n",
        "              'lon': lon,\n",
        "              'type': 'pf',\n",
        "              'data': 'depth',\n",
        "              'units': 'english',\n",
        "              'series': 'pds'\n",
        "          }\n",
        "      except AttributeError:\n",
        "          print(f\"Empty dataframe created for {site}.  Perhaps there is no ARI data for this location\")\n",
        "          continue\n",
        "\n",
        "      full_url = requests.Request('GET', atlas_url, params=params).prepare().url\n",
        "      # Construct the full URL with parameters\n",
        "      print(f\"Full URL for {site}: {full_url}\")\n",
        "      # Make the API request\n",
        "      response = requests.get(full_url, verify=False)\n",
        "\n",
        "      # Check if the request was successful\n",
        "      if response.status_code == 200:\n",
        "          # Split the response text to isolate the variable assignments\n",
        "        lines = response.text.split(';')\n",
        "\n",
        "        # Dictionary to store parsed data\n",
        "        parsed_data = {}\n",
        "\n",
        "        # Parse each variable assignment\n",
        "        for line in lines:\n",
        "            if '=' in line:\n",
        "                key, value = line.split('=', 1)\n",
        "                key = key.strip()\n",
        "                value = value.strip()\n",
        "                try:\n",
        "                    # Safely evaluate the value using ast.literal_eval()\n",
        "                    parsed_data[key] = ast.literal_eval(value)\n",
        "                except (ValueError, SyntaxError):\n",
        "                    # If the value cannot be parsed, keep it as a string\n",
        "                    parsed_data[key] = value\n",
        "\n",
        "        # Access the parsed data, for example:\n",
        "        result = parsed_data.get('result')\n",
        "        print(result)\n",
        "        if result == 'none' or result == 'null':\n",
        "          continue\n",
        "        else:\n",
        "          quantiles = parsed_data.get('quantiles')\n",
        "          #loop through the quantiles\n",
        "          data_dict['FID'].append(new_fid)\n",
        "          data_dict['Site'].append(site.upper())\n",
        "          data_dict['Lat'].append(lat)\n",
        "          data_dict['Lon'].append(lon)\n",
        "          for key in interval_dict:\n",
        "            for i, ari in enumerate(recurrence_dict):\n",
        "              header = f'F{recurrence_dict[ari]}{interval_dict[key]}ARI'\n",
        "              data_dict[header].append(quantiles[key][i])\n",
        "      else:\n",
        "          print(f\"Error: {response.status_code}\")\n",
        "      new_ari_df = pd.DataFrame(data_dict)\n",
        "      updated_ari_df= pd.concat([old_ari_df, new_ari_df], ignore_index=True)\n",
        "      updated_ari_df.to_csv(ari_output, index=False)\n",
        "      print(f\"Done with extracting ARI data.  {ari_output} can be found in the current working directory.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JLdCf63WUlLl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3. Select End Date (Valid Time)**\n",
        "Select the valid date/time you want to view the snapshot on the tool.  Date/time format has to be pretty specific so make sure its formatted correctly. Valid times are always in UTC.  You will also need to enter the log-in credentials for the AGOL account (NWS Juneau is where the tool resides).\n",
        "\n",
        "After this cell finishes running (assuming you have input the correct AGOL credentials), you can check the output at: https://noaa.maps.arcgis.com/apps/dashboards/a4857d02205247a2965b1ad3c5cc369f"
      ],
      "metadata": {
        "id": "E-h6rGeDCvFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Created on Tue Jan 26 16:54:08 2021\n",
        "\n",
        "@author: David Levin\n",
        "\"\"\"\n",
        "\n",
        "#@markdown **Enter your valid time in \"YYYYmmddhhmm\" format below**\n",
        "END = '' #@param {type:\"string\"}\n",
        "START = datetime.strptime(END, '%Y%m%d%H%M') - timedelta(hours=24)\n",
        "START = START.strftime('%Y%m%d%H%M')\n",
        "###################### Config for the Precipitation Script ###################\n",
        "\n",
        "###################### MesoWest API Config ###################################\n",
        "# The state we want to pull MesoWest data from\n",
        "STATE = 'ak'\n",
        "\n",
        "# To search for the last two hours of observations; recent=120.\n",
        "RECENT = '180'\n",
        "\n",
        "# pmode (totals, intervals, last), defines the interval mode to calculate precipitation.\n",
        "# If omitted the returned JSON formatting will be significantly different.\n",
        "PMODE = 'last'\n",
        "# pmode=intervals, Returns accumulated precipitation for intervals provided in\n",
        "# the additional interval argument. Valid keywords for interval are hour, day,\n",
        "# week, month, year, or non-zero integer in hours. Integers must be a factor or\n",
        "# multiple of 24 (1,2,3,4,6,8,12,24,48,72,etc). Default value is day if\n",
        "# interval is not provided. Partial intervals at the end of a requested range\n",
        "# are still returned. Note that all keywords or integers provided to interval\n",
        "# will use UTC time zone to define the start and end of each interval.\n",
        "# However, each interval respects the requested start hour such that intervals\n",
        "# can be offset for a local time zone.\n",
        "INTERVALS = ['1', '3', '6', '12', '24', '48', '72']\n",
        "\n",
        "# You will need a MesoWest API account.  It's free and you will receive your own token\n",
        "# for downloading data.  An example is included below.\n",
        "TOKEN = token\n",
        "\n",
        "######################## File Paths ###########################################\n",
        "\n",
        "# Where you want your .csv file to go...change this!\n",
        "# Could also add an upload method and have it upload to the web for better\n",
        "# linkage with Arc Online\n",
        "OBS_PATH = '/nas/nomad/ARITool'\n",
        "\n",
        "# Check if the directory exists\n",
        "print(\"Now setting up directories\")\n",
        "if not os.path.exists(OBS_PATH):\n",
        "    # If it doesn't exist, create it\n",
        "    os.makedirs(OBS_PATH)\n",
        "    print(f\"Directory '{OBS_PATH}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Directory '{OBS_PATH}' already exists.\")\n",
        "\n",
        "\n",
        "\n",
        "# Whatever you want to call your precip file (these should be stored in the OBS_PATH directory)\n",
        "PRECIP_OBS_FILE = 'LatestAKPrecipTEST.csv'\n",
        "# The file name of the merged precip data and bad station data\n",
        "FINAL_PRECIP_FILE = 'LatestAKPrecip_BadStations.csv'\n",
        "# The list of your static ARI .csv files which should be in the same\n",
        "# directory as your other data (OBS_PATH)\n",
        "ARI_FILE = 'TotalARI_Extract_Version3.csv'\n",
        "# This is the master file which will eventually overwrite the hosted feature layer\n",
        "# on the ArcGis Online server.\n",
        "FINAL_OUTPUT_FILE = 'LatestPrecip_ARITotal_Retrospective_TEST.csv'\n",
        "# Where you would like your log file to be placed\n",
        "LOG_PATH = '/nas/nomad/ARITool'\n",
        "# Name of your log file\n",
        "LOG_FILE = 'meso_west.log'\n",
        "\n",
        "try:\n",
        "  print(\"Now moving some files around to get started...\")\n",
        "  # Source file path (assuming it's in the current working directory)\n",
        "  source_file = ARI_FILE\n",
        "\n",
        "  # Destination directory path\n",
        "  destination_dir = '/nas/nomad/ARITool'\n",
        "\n",
        "  # Destination file path\n",
        "  destination_file = os.path.join(destination_dir, source_file)\n",
        "\n",
        "  # Move the file\n",
        "  shutil.move(source_file, destination_file)\n",
        "\n",
        "  print(f\"File moved successfully to {destination_file}\")\n",
        "except FileNotFoundError:\n",
        "  print(f\"Source file '{source_file}' not found in working directory.\")\n",
        "  print(f\"Will check {OBS_PATH}\")\n",
        "  if os.path.exists(os.path.join(OBS_PATH, ARI_FILE)):\n",
        "    print(f\"{ARI_FILE}' already exists in {OBS_PATH}\")\n",
        "  else:\n",
        "    raise FileNotFoundError(f\"{ARI_FILE} not found in {OBS_PATH}.  Run cell 2 one more time!\")\n",
        "\n",
        "\n",
        "###################### Output Headers & Formating #############################\n",
        "\n",
        "# initializing our output for the .csv file we will generate\n",
        "# Just make sure the headers for the variables are in the same order as the\n",
        "# INTERVALS list above\n",
        "PRECIP_OUTPUT = 'FID,Site,Lat,Lon,DateTime,1hr_Precip,3hr_Precip,6hr_Precip,'\n",
        "PRECIP_OUTPUT += '12hr_Precip,24hr_Precip,48hr_Precip,72hr_Precip\\n'\n",
        "\n",
        "# A list of redundant columns after merging the ARI and MesoWest dataframes\n",
        "# Probably won't have to change this\n",
        "BAD_COLUMNS = ['FID_y', 'Lat_y', 'Lon_y']\n",
        "# for dropping NaN values by column name\n",
        "ARI_HOURS = ['72', '48', '24', '12', '6', '3', '1']\n",
        "\n",
        "# A list of the return intevals you would like to be calculated\n",
        "ARI_LIST = ['1000', '500', '200', '100', '50', '25', '10', '5', '2', '1']\n",
        "\n",
        "#################### ArcGis Login & URLs #####################################\n",
        "#@markdown Login credentials for AGOL.  Make sure to use your office\n",
        "#@markdown Enterprise account login info so that you have full permissions\n",
        "AGOL_USER = '' #@param {type:\"string\"}\n",
        "\n",
        "AGOL_PASSWORD = '' #@param {type:\"string\"}\n",
        "# Url for the hosted feature layer which will be overwritten.  You can grab\n",
        "# this by copying the url after opening the hosted feature layer in the NOAA\n",
        "# Enterprise Account for your office under \"Content\"\n",
        "# Note that this hosted layer has to actually exist for this tool to work.\n",
        "# You will need to manually upload the .csv file the first time. After that, this\n",
        "# should be automated as you will have a url generated after the first upload.\n",
        "AGOL_URL = 'https://services2.arcgis.com/C8EMgrsFcRFL6LrL/arcgis/rest/services/'\n",
        "AGOL_URL += 'LatestPrecip_ARITotal_Retrospective_TEST/FeatureServer'\n",
        "AGOL_NAME = 'LatestPrecip_ARITotal_Retrospective_TEST'\n",
        "#https://services2.arcgis.com/C8EMgrsFcRFL6LrL/arcgis/rest/services/LatestPrecip_ARITotal_Retrospective_TEST/FeatureServer\n",
        "\n",
        "###################### Config for Flagging Bad Stations ######################\n",
        "# URL for the google sheet RFC publishes as a web service\n",
        "STN_URL = 'https://script.googleusercontent.com/macros/echo?user_content_key='\n",
        "STN_URL += '5J_H9sc27K5WgpKbXmbm_zG9LirVMIUFRIggn7LFHkkCbDZU2csoUZH5mTS2uXmfTiv'\n",
        "STN_URL += 'ywganm1M-nO4EO14d-FxMMk8D-Ep5m5_BxDlH2jW0nuo2oDemN9CCS2h10ox_1xSnc'\n",
        "STN_URL += 'GQajx_ryfhECjZEnG-eZm-jLUSFG87Bnj5J5AjenQfNssDMPC4AmEKfQvHjFVxZhIML'\n",
        "STN_URL += 'mTS8mzDTCqnGIg&lib=MXTfKjftoipE0WwmBFaK8ZA0VGsytdp8v'\n",
        "\n",
        "# Columns from the APRFC spreadsheet you want to keep\n",
        "# I chose the \"Status\", \"Status as of\", and \"Notes\" columns\n",
        "COL_INDEX = ['Status', 'Status as of', 'Notes', 'Bad_Obs', 'Good_Obs']\n",
        "# File to which you are writing your text data from the url above\n",
        "BAD_STN_FILE = 'Bad_Stations.txt'\n",
        "# where your parsed data gets placed from the data in the text file above\n",
        "BAD_STN_SHEET = 'APRFC_Bad_Stations.csv'\n",
        "\n",
        "########################## Methods ##############################################\n",
        "\n",
        "def download_precip(start, end, state, recent, mode, ints, token):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    state : The state abbreviation (lower case) for\n",
        "        which you are requesting data.\n",
        "    recent : Number of minutes to look back for new obs from the most recent\n",
        "        time. 180 would be looking back 3 hrs for instance.\n",
        "    mode : There are two modes--intervals and last.  Intervals returns precip\n",
        "        for a requested time interval.  Last returns precip based on the latest\n",
        "        time and you can use 'accum_intervals' to specify a list of intervals\n",
        "        based on this time.  Last is preferred and this is how the method is\n",
        "        set up.\n",
        "    ints : A list of the time intervals for which to pull precip data in hours\n",
        "        (i.e. 1,3,6,12,24)\n",
        "    token : Your MesoWest API\n",
        "        token.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    data : A self describing JSON\n",
        "        object from the MesoWest API.\n",
        "\n",
        "    \"\"\"\n",
        "    n = 0\n",
        "    # Initializing our url\n",
        "    #url = 'https://api.synopticdata.com/v2/stations/precip?'\n",
        "    #url += 'state='+state+'&pmode='+mode+'&recent='+recent+'&accum_hours='\n",
        "    url = 'https://api.synopticdata.com/v2/stations/precip?'\n",
        "    url+='state='+state+'&pmode='+mode+'&start='+start+'&end='+end+'&accum_hours='\n",
        "    # To be tacked on after we add our vars\n",
        "    endurl = '&units=english&output=json&token='+token\n",
        "    # adding our vars\n",
        "    while n < len(ints):\n",
        "        if n != len(ints)-1:\n",
        "            url += ints[n]+','\n",
        "        else:\n",
        "            url += ints[n]\n",
        "        n = n+1\n",
        "    # now adding the end of the url\n",
        "    url = url+endurl\n",
        "    # now requesting the data\n",
        "    page = urllib.request.urlopen(url)\n",
        "    data = page.read()\n",
        "\n",
        "    return data\n",
        "\n",
        "def parse_json(data):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : A self describing\n",
        "        JSON object from MesoWest API\n",
        "    Returns\n",
        "    -------\n",
        "    json_dict : A python dictionary\n",
        "        created from the JSON object.\n",
        "    \"\"\"\n",
        "    # Converting from json to python dictionary\n",
        "    json_dict = json.loads(data)\n",
        "    return json_dict\n",
        "\n",
        "def parse_precip(precip_output, json_dict):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    precip_output : An output string of headers for your precip data which is\n",
        "    pulled from the config file\n",
        "    json_dict : A python dictionary created from a MesoWest JSON query\n",
        "    Returns\n",
        "    -------\n",
        "    precip_output : The original output string with all the data organized and\n",
        "                    added into a comma separated format\n",
        "    \"\"\"\n",
        "    for i in range(0, len(json_dict['STATION'])):\n",
        "        # now pulling the data out of the massive dictionary by looping through the stations\n",
        "        ob = json_dict['STATION'][i]\n",
        "        # ArcGIS nees an FID field to plot so making one up with i\n",
        "        FID = str(i)\n",
        "        site = ob['STID']\n",
        "        #print(site)\n",
        "        lat = ob['LATITUDE']\n",
        "        lon = ob['LONGITUDE']\n",
        "        datetime = ob['OBSERVATIONS']['precipitation'][0]['last_report']\n",
        "        pdata = ob['OBSERVATIONS']['precipitation']\n",
        "        # datetime is the last entry in the \"PERIOD_OF_RECORD\" dictionary\n",
        "        #datetime = ob['PERIOD_OF_RECORD']['end']\n",
        "        precip_output += FID+','+site+','+lat+','+lon+','+datetime+','\n",
        "        # Checking to make sure our precip intervals are there, if no precip\n",
        "        # is reported those intervals are blank in the json data\n",
        "        # if all the intervals are there the length of the list will be 7\n",
        "        # else we need to check for missing intervals\n",
        "        if len(pdata) == 7:\n",
        "            for count, value in enumerate(pdata):\n",
        "                if count != 6:\n",
        "                    # Getting rid of traces that show up as 0.001\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+','\n",
        "                    else:\n",
        "                        precip_output += '0.00,'\n",
        "                else:\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+'\\n'\n",
        "                    else:\n",
        "                        precip_output += '0.00\\n'\n",
        "        elif len(pdata) == 6:\n",
        "            precip_output += ','\n",
        "            for count, value in enumerate(pdata):\n",
        "                if count != 5:\n",
        "                    # Getting rid of traces that show up as 0.001\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+','\n",
        "                    else:\n",
        "                        precip_output += '0.00,'\n",
        "                else:\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+'\\n'\n",
        "                    else:\n",
        "                        precip_output += '0.00\\n'\n",
        "        elif len(pdata) == 5:\n",
        "            precip_output += ',,'\n",
        "            for count, value in enumerate(pdata):\n",
        "                if count != 4:\n",
        "                    # Getting rid of traces that show up as 0.001\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+','\n",
        "                    else:\n",
        "                        precip_output += '0.00,'\n",
        "                else:\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+'\\n'\n",
        "                    else:\n",
        "                        precip_output += '0.00\\n'\n",
        "        elif len(pdata) == 4:\n",
        "            precip_output += ',,,'\n",
        "            for count, value in enumerate(pdata):\n",
        "                if count != 3:\n",
        "                    # Getting rid of traces that show up as 0.001\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+','\n",
        "                    else:\n",
        "                        precip_output += '0.00,'\n",
        "                else:\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+'\\n'\n",
        "                    else:\n",
        "                        precip_output += '0.00\\n'\n",
        "        elif len(pdata) == 3:\n",
        "            precip_output += ',,,,'\n",
        "            for count, value in enumerate(pdata):\n",
        "                if count != 2:\n",
        "                    # Getting rid of traces that show up as 0.001\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+','\n",
        "                    else:\n",
        "                        precip_output += '0.00,'\n",
        "                else:\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+'\\n'\n",
        "                    else:\n",
        "                        precip_output += '0.00\\n'\n",
        "        elif len(pdata) == 2:\n",
        "            precip_output += ',,,,,'\n",
        "            for count, value in enumerate(pdata):\n",
        "                if count != 1:\n",
        "                    # Getting rid of traces that show up as 0.001\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+','\n",
        "                    else:\n",
        "                        precip_output += '0.00,'\n",
        "                else:\n",
        "                    if value['total'] >= 0.01:\n",
        "                        precip_output += str(round(value['total'], 2))+'\\n'\n",
        "                    else:\n",
        "                        precip_output += '0.00\\n'\n",
        "        elif len(pdata) == 1:\n",
        "            precip_output += ',,,,,,'\n",
        "            for count, value in enumerate(pdata):\n",
        "                if value['total'] >= 0.01:\n",
        "                    precip_output += str(round(value['total'], 2))+'\\n'\n",
        "                else:\n",
        "                    precip_output += '0.00\\n'\n",
        "    return precip_output\n",
        "\n",
        "def grab_bad_stnlist(url):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    url : a url from which you wish to scrape data\n",
        "    Returns\n",
        "    -------\n",
        "    data : the data from your urllib.request object\n",
        "\n",
        "    \"\"\"\n",
        "    # now requesting the bad station list from APRFC\n",
        "    page = urllib.request.urlopen(url)\n",
        "    data = page.read()\n",
        "\n",
        "    return data\n",
        "\n",
        "def write_to_file(data, path, fname, binary=True):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : raw data (such as from a web scrape)\n",
        "    path : where you want your data stored\n",
        "    fname : name of your output file\n",
        "    binary : write mode of 'wb' if True and 'w' if False\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    \"\"\"\n",
        "    if binary:\n",
        "        writeflag = 'wb'\n",
        "    else:\n",
        "        writeflag = 'w'\n",
        "    outfile = open(os.path.join(path, fname), writeflag)\n",
        "    outfile.write(data)\n",
        "    outfile.close()\n",
        "\n",
        "def format_data(path, fname):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    path : path to your data\n",
        "    fname : name of your text file\n",
        "    Returns\n",
        "    -------\n",
        "    dict_fm_file : formatted data (python dictionary/list from your text data)\n",
        "\n",
        "    \"\"\"\n",
        "    with open(os.path.join(path, fname)) as newfile:\n",
        "        #dict_fm_file = eval(newfile.read())\n",
        "        dict_fm_file = json.loads(newfile.read())\n",
        "        newfile.close()\n",
        "\n",
        "    return dict_fm_file\n",
        "\n",
        "def build_badstn_dict(station_list):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    station_list : a python list of dictionaries formatted from the APRFC web\n",
        "        service google sheet of bad station data\n",
        "    Returns\n",
        "    -------\n",
        "    stn_dict : reformats the data into a dictionary with the keys being\n",
        "        column headers in the original spreadsheet and the values being a list\n",
        "        containing the column values\n",
        "\n",
        "    \"\"\"\n",
        "    stn_dict = {}\n",
        "    stn_keys = station_list[0].keys()\n",
        "    for key in stn_keys:\n",
        "        values_list = []\n",
        "        for stn in station_list:\n",
        "            values_list.append(stn[key])\n",
        "        stn_dict.update({key:values_list})\n",
        "    return stn_dict\n",
        "\n",
        "def create_badstn_dataframe(df_dict, drop_col, drop_dupes=True):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_dict : a python dictionary from which you want a dataframe\n",
        "    drop_col : A column (string) you wish to search for duplicate values\n",
        "    drop_dupes : if set to True will drop all rows that have duplicate values\n",
        "    in the drop_col variable\n",
        "    Returns\n",
        "    -------\n",
        "    newdf : a pandas dataframe\n",
        "\n",
        "    \"\"\"\n",
        "    newdf = pd.DataFrame(df_dict)\n",
        "    if drop_dupes:\n",
        "        newdf.drop_duplicates(drop_col, inplace=True)\n",
        "        newdf.reset_index(inplace=True)\n",
        "    else:\n",
        "        pass\n",
        "    return newdf\n",
        "\n",
        "def save_badstn_dataframe(df, path, dfname, cols_to_keep, col_to_drop,\n",
        "                          dropcols=True, dropindex=True, keep_cols=True):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : a pandas dataframe you wish to save as a .csv\n",
        "    path : where you want your file saved\n",
        "    dfname : what you want to call your file\n",
        "    cols_to_keep : a list of columns you wish to save (can be an empty list,\n",
        "                                                       just set keep_cols to False)\n",
        "    col_to_drop : A single column (string) you wish to drop from your dataframe.  Can be\n",
        "    empty string.\n",
        "    dropcols : You can choose to drop a single column if true.  Otherwise\n",
        "    better just to use cols_to_keep with keep_cols = True\n",
        "    dropindex : Deleting the index column if desired (True)\n",
        "    keep_cols : keeps the columns passed in the cols_to_keep list if True\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    \"\"\"\n",
        "    #print(\"Dropping this column %s\" %(col_to_drop))\n",
        "    #print(\"Keeping these columns %s\" %(cols_to_keep))\n",
        "    if dropcols:\n",
        "        final_df = df.drop([col_to_drop], axis=1)\n",
        "    else:\n",
        "        final_df = df\n",
        "    if dropindex and keep_cols:\n",
        "        final_df = final_df.reindex(columns=cols_to_keep)\n",
        "        final_df.to_csv(os.path.join(path, dfname), columns=cols_to_keep, index=False)\n",
        "    elif dropindex and not keep_cols:\n",
        "        final_df.to_csv(os.path.join(path, dfname), index=False)\n",
        "    elif not dropindex and keep_cols:\n",
        "        final_df = final_df.reindex(columns=cols_to_keep)\n",
        "        final_df.to_csv(os.path.join(path, dfname), columns=cols_to_keep, index=False)\n",
        "    elif not dropindex and not keep_cols:\n",
        "        final_df.to_csv(os.path.join(path, dfname))\n",
        "\n",
        "def merge_badstn_dataframe(df1, df2, path, stn_columns, merge_how, left_merge, right_merge):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df1 : the primary .csv file (keeping all data)\n",
        "    df2 : secondary .csv file (you are merging this data with df1)\n",
        "    stn_columns : a list of columns you wish to keep from the stn_data df (can be empty)\n",
        "    how : How you want to merge (\"left\" is preferred but if you choose \"right\" you will keep\n",
        "                                 all data from df2 instead)\n",
        "    left_on : Column in df1 that you wish to merge on\n",
        "    right_on : Column in df2 that you wish to merge on\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    added_df : A merge of df1 and df2 with dummy columns added for bad and good obs\n",
        "    new_cols : A list of columns you wish to keep in your final df\n",
        "\n",
        "    \"\"\"\n",
        "    first_df = pd.read_csv(os.path.join(path, df1))\n",
        "    second_df = pd.read_csv(os.path.join(path, df2))\n",
        "    cols_to_keep = first_df.columns.tolist()\n",
        "    new_cols = cols_to_keep + stn_columns\n",
        "    combo_df = first_df.merge(second_df, how=merge_how,\n",
        "                              left_on=left_merge, right_on=right_merge)\n",
        "    added_df = add_dummy_cols(combo_df)\n",
        "    return added_df, new_cols\n",
        "\n",
        "def add_dummy_cols(df1):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df1 : A Pandas dataframe of bad station data from APRFC.\n",
        "    Returns\n",
        "    -------\n",
        "    df1 : The same dataframe with columns added for the gauge status\n",
        "    \"\"\"\n",
        "    df1['Bad_Obs'] = np.where(df1['Status'] == 'Bad', 'Bad', '')\n",
        "    df1['Good_Obs'] = np.where(df1['Status'] != 'Bad', 'Good', '')\n",
        "    return df1\n",
        "\n",
        "def merge_csv(path, first, second, cols_to_del, hour):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path : The path for your precip data and your ARI (should be in the same location)\n",
        "    first : An input .csv file containing the ARI data extracted from PFDS\n",
        "    second : An input .csv file containing the latest MesoWest Precip obs\n",
        "    col_to_del : A list of redundant columns you wish to delete after merging\n",
        "            usually lat/lon and FID and can be read from config\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    final_df : A pandas dataframe containing the merged MesoWest precip data\n",
        "            and the ARI data\n",
        "\n",
        "    \"\"\"\n",
        "    df1 = pd.read_csv(os.path.join(path, first))\n",
        "    df2 = pd.read_csv(os.path.join(path, second))\n",
        "    combined_df = df1.merge(df2, how='left', left_on='Site', right_on='Site', suffixes=('', '_y'))\n",
        "    # dropping stations which don't have ARIs\n",
        "    clean_merged_df = combined_df.dropna(subset=['F1000yr'+hour+'hrARI'])\n",
        "    final_df = clean_merged_df.drop(columns=cols_to_del)\n",
        "    return final_df\n",
        "\n",
        "def calc_all_percent_exceedance(path, infile, hours, ri_list):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path : The path for your precip data and your ARI (should be in the same location)\n",
        "    infile : An input .csv file containing the merged MesoWest precip obs and\n",
        "            ARI data from PFDS\n",
        "    hours : A list of the time intervals for the accumulated precip (strings such as: '24','12')\n",
        "    ri_list : A list of the ARI for which you want percent\n",
        "           exceedances (1, 2, 5, 10 year ARI etc)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    clean_exceedance_df : A pandas dataframe containing the merged MesoWest precip data\n",
        "            and the ARI data with percent exceedances calculated and the RIs\n",
        "            dropped to clean up the file a bit\n",
        "\n",
        "    \"\"\"\n",
        "    exceedance_df = pd.read_csv(os.path.join(path, infile))\n",
        "    # calculating % exceedance at each precip interval, for each RI at that interval\n",
        "    # initializing a list of RI columns we can now drop since we no longer need them\n",
        "    drop_list = []\n",
        "    for hr in hours:\n",
        "        for col in ri_list:\n",
        "            exceedance_df[col+'yr_'+hr+'hr_PercentExceedance'] = \\\n",
        "            round((exceedance_df[hr+'hr_Precip']/exceedance_df['F'+col+'yr'+hr+'hrARI'])*100, 0)\n",
        "            drop_list.append('F'+col+'yr'+hr+'hrARI')\n",
        "    # Now dropping the RI columns as we don't need them any more\n",
        "    clean_exceedance_df = exceedance_df.drop(drop_list, axis=1)\n",
        "    return clean_exceedance_df\n",
        "\n",
        "# Need to pass an hours list\n",
        "def calc_all_ari(path, infile, hours, ri_list):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path : The path for your precip data and your ARI (should be in the same location)\n",
        "    infile : An input .csv file containing the merged MesoWest precip obs and\n",
        "            ARI data from PFDS along with the calculated % exceedance (important!)\n",
        "    hours : A list of the time intervals for the accumulated precip (string such as: '24','12')\n",
        "    ri_list : A list of the ARI for which you want percent\n",
        "           exceedances (1, 2, 5, 10 year ARI etc)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ri_df : A pandas dataframe containing the merged MesoWest precip data\n",
        "            and the ARI data with the ARI calculated for each ob\n",
        "\n",
        "    \"\"\"\n",
        "    # opening our file\n",
        "    ri_df = pd.read_csv(os.path.join(path, infile))\n",
        "    # Looping through the precip accumulation intervals\n",
        "    for hr in hours:\n",
        "        # Initiating our new columns\n",
        "        col_names = []\n",
        "        # Now naming our columns from the list of ARIs\n",
        "        for col in ri_list:\n",
        "            col_names.append(col+'yr_'+hr+'hr_PercentExceedance')\n",
        "        #Initializing our list of conditions to check our data against\n",
        "        conditions = []\n",
        "        # Initializing the list of values we want to place in our RI column\n",
        "        # when certain conditions are met\n",
        "        output = []\n",
        "        # We want to check each percent exceedance column for values > 100%\n",
        "        for value in col_names:\n",
        "            conditions.append(ri_df[value] >= 100)\n",
        "        # We want to fill our RI column with the return interval that corresponds\n",
        "        # to the met condition\n",
        "        for ari in ri_list:\n",
        "            output.append(float(ari))\n",
        "        #print('Output is: ')\n",
        "        #print(output)\n",
        "        # Now for amounts that do not exceed any RI, we have a seperate condition\n",
        "        conditions.append(ri_df[col_names[len(col_names)-1]] < 100)\n",
        "        # For this, we want to append the actual percent exceedance/100\n",
        "        output.append(round(ri_df[col_names[9]].div(100), 2))\n",
        "        # Now we add a new column to our dataframe with the calculated output\n",
        "        # values generated from our conditions\n",
        "        ri_df[hr+'hr_RI'] = np.select(conditions, output)\n",
        "    # calculating which precip accumulation interval has the highest RI\n",
        "    ri_df['Max_RI'] = ri_df[['72hr_RI','48hr_RI','24hr_RI','12hr_RI','6hr_RI','3hr_RI','1hr_RI']].max(axis=1)\n",
        "    return ri_df\n",
        "\n",
        "def replace_gis_file(user, pw, path, myfile, agol_name):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    user : Your AGOL username (string)\n",
        "    pw : Your AGOL password (string)\n",
        "    path : The file path where your .csv file is stored (string)\n",
        "    myfile : The filename of your .csv file (string)\n",
        "    agol_name : The name of your hosted feature layer (should be the filename\n",
        "                                                       without the extention\n",
        "                                                       i.e. 'AK_Obs' instead\n",
        "                                                       of 'AK_Obs.csv')\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    # GIS logs pretty much everything for you so no need to call logger here\n",
        "    # Now logging into Arc Online to update the hosted layer\n",
        "    gis = GIS('https://noaa.maps.arcgis.com/home', username=user, password=pw)\n",
        "\n",
        "    try:\n",
        "        data_file   = os.path.join(path, myfile)\n",
        "        #### Delete prior to reposting ####\n",
        "        item_types = [\"CSV\", \"Feature Layer Collection\"]\n",
        "        name_list = [agol_name]\n",
        "        for current_item_type in item_types:\n",
        "            for file_name in name_list:\n",
        "                search_result = gis.content.search(query=file_name, item_type=current_item_type)\n",
        "                if len(search_result) > 0:\n",
        "                    for item in search_result:\n",
        "                        item.delete()\n",
        "                        print(\"Deleted existing \" + current_item_type + \": \", item)\n",
        "\n",
        "        #### Replace file ####\n",
        "        csv_item   = gis.content.add({}, data_file)\n",
        "\n",
        "        #### Re-Publish File ####\n",
        "        primary_feature_layer   = csv_item.publish()\n",
        "\n",
        "        print (primary_feature_layer.url)\n",
        "\n",
        "    except IOError:\n",
        "        print(\"GIS error\")\n",
        "\n",
        "def do_some_gis(user, pw, path, myurl, myfile):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    user : Your AGOL username (string)\n",
        "    pw : Your AGOL password (string)\n",
        "    path : The file path where your .csv file is stored (string)\n",
        "    myurl : The URL of your hosted feature layer service from AGOL (string)\n",
        "    myfile : The filename of your .csv file (string)\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    # GIS logs pretty much everything for you so no need to call logger here\n",
        "    # Now logging into Arc Online to update the hosted layer\n",
        "    login = GIS('https://noaa.maps.arcgis.com/home', username=user, password=pw)\n",
        "    # Accessing my feature layer collection by using the url generated from\n",
        "    # within AGOL.\n",
        "    my_content = myurl\n",
        "    # Use the url to create a FeatureLayerCollection object which you will overwrite with the\n",
        "    # new data.\n",
        "    akobs = FeatureLayerCollection(my_content, login)\n",
        "    # Now overwriting the old .csv file with the new one.\n",
        "    # Make sure your obs_path and filenames match\n",
        "    akobs.manager.overwrite(os.path.join(path, myfile))\n",
        "\n",
        "def create_hex_colors(ripath, rifile):\n",
        "    ri_columns = ['1hr_RI','3hr_RI','6hr_RI','12hr_RI','24hr_RI','48hr_RI','72hr_RI','Max_RI']\n",
        "    new_columns = ['1hrRI_Color','3hrRI_Color','6hrRI_Color','12hrRI_Color','24hrRI_Color',\n",
        "                   '48hrRI_Color','72hrRI_Color','MaxRI_Color']\n",
        "    ridf = pd.read_csv(os.path.join(ripath, rifile))\n",
        "    for count, col in enumerate(new_columns):\n",
        "        print(ri_columns[count])\n",
        "        conditions = [ridf[ri_columns[count]] <= 1, ridf[ri_columns[count]] == 2, ridf[ri_columns[count]] == 5,\n",
        "                      ridf[ri_columns[count]] == 10, ridf[ri_columns[count]] == 25, ridf[ri_columns[count]] == 50,\n",
        "                      ridf[ri_columns[count]] == 100, ridf[ri_columns[count]] == 200, ridf[ri_columns[count]] == 500,\n",
        "                      ridf[ri_columns[count]] == 1000, ridf[ri_columns[count]] == 2000]\n",
        "        values = ['#55FF00','#FFFF00','#FFAA00','#FF0000','#FF99FF','#FF00FF','#660066','#FFFFFF',\n",
        "                  '#FFFFFF', '#FFFFFF', '#FFFFFF']\n",
        "        ridf[col] = np.select(conditions, values)\n",
        "    return ridf\n",
        "######################## Main Code ##############################################\n",
        "\n",
        "def execute(START, END):\n",
        "    # Setting up our logging\n",
        "    # using the default root logger\n",
        "    for handler in logging.root.handlers[:]:\n",
        "        logging.root.removeHandler(handler)\n",
        "    # Configuring our log\n",
        "    logging.basicConfig(filename=os.path.join(LOG_PATH, LOG_FILE), filemode='w',\n",
        "                        format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S',\n",
        "                        level=logging.DEBUG)\n",
        "    # Grabbing our logger\n",
        "    logger = logging.getLogger('')\n",
        "\n",
        "    # grabbing our data\n",
        "    precip_data = download_precip(START, END, STATE, RECENT, PMODE, INTERVALS, TOKEN)\n",
        "    logger.info('Grabbed JSON object from MesoWest')\n",
        "\n",
        "    # parsing the raw JSON into a python-readable dictionary\n",
        "    parsed_precip = parse_json(precip_data)\n",
        "    logger.info('Now checking to see if we have a valid query')\n",
        "    # Checking to see if we made a valid request from the MesoWest API for precip\n",
        "    if parsed_precip['SUMMARY']['NUMBER_OF_OBJECTS'] != 0:\n",
        "        logger.info('Found valid MesoWest API query')\n",
        "    else:\n",
        "        # printing the error message from the JSON object to the log file\n",
        "        logger.warning('Invalid JSON request! %s', parsed_precip['SUMMARY']['RESPONSE_MESSAGE'])\n",
        "        #sys.exit()\n",
        "    logger.info('Now looping through the stations')\n",
        "    # Creating the final precip output string\n",
        "    final_output = parse_precip(PRECIP_OUTPUT, parsed_precip)\n",
        "    # Writing the output to the PRECIP_OBS_FILE\n",
        "    f = open(os.path.join(OBS_PATH, PRECIP_OBS_FILE), 'w')\n",
        "    f.write(final_output)\n",
        "    f.close()\n",
        "    logger.info('Final output now written to %s', PRECIP_OBS_FILE)\n",
        "    # Now grabbing bad stations from APRFC and appending to the precip file\n",
        "    logger.info('Now grabbing the bad station list from APRFC')\n",
        "    write_to_file(grab_bad_stnlist(STN_URL), OBS_PATH, BAD_STN_FILE)\n",
        "    # Formatting the raw text into a python list\n",
        "    stn_data = format_data(OBS_PATH, BAD_STN_FILE)\n",
        "    # reformating the list into a dictionary for ease of transition into pandas\n",
        "    station_dictionary = build_badstn_dict(stn_data)\n",
        "    # Creating our dataframe from the bad station data\n",
        "    station_dataframe = create_badstn_dataframe(station_dictionary, 'Location ID')\n",
        "    # Saving the file\n",
        "    logger.info('Now saving the bad station data as %s', BAD_STN_SHEET)\n",
        "    save_badstn_dataframe(station_dataframe, OBS_PATH, BAD_STN_SHEET,\n",
        "                          station_dataframe.columns, 'Row', keep_cols=False)\n",
        "    # Now merging it with the precip file\n",
        "    logger.info('Now merging precip data with the bad station list from APRFC')\n",
        "    merged_df, columns = merge_badstn_dataframe(PRECIP_OBS_FILE, BAD_STN_SHEET,\n",
        "                                                OBS_PATH, COL_INDEX, 'left', 'Site', 'Location ID')\n",
        "    # Now saving the precip file with the bad station data appended\n",
        "    save_badstn_dataframe(merged_df, OBS_PATH, FINAL_PRECIP_FILE, columns, '', dropcols=False)\n",
        "    logger.info('All done updating %s with bad stations!', FINAL_PRECIP_FILE)\n",
        "    print(\"All Done Updating Bad Stations!\")\n",
        "    # Merging the precip output with the ARI file\n",
        "    logger.info('Now merging precip data with the ARIs')\n",
        "    # # No longer a loop through multiple ARI files\n",
        "    # Added index = False on the to_csv call to remove the index column\n",
        "    precip_df = merge_csv(OBS_PATH, FINAL_PRECIP_FILE, ARI_FILE, BAD_COLUMNS, '24')\n",
        "    precip_df.to_csv(os.path.join(OBS_PATH, FINAL_OUTPUT_FILE), index=False)\n",
        "    # Calculating the percent exceedances for each precip interval at each station\n",
        "    ex_df = calc_all_percent_exceedance(OBS_PATH, FINAL_OUTPUT_FILE, ARI_HOURS, ARI_LIST)\n",
        "    ex_df.to_csv(os.path.join(OBS_PATH, FINAL_OUTPUT_FILE), index=False)\n",
        "    # Calculating the return intervals for each precip interval at each station\n",
        "    ari_df = calc_all_ari(OBS_PATH, FINAL_OUTPUT_FILE, ARI_HOURS, ARI_LIST)\n",
        "    ari_df.to_csv(os.path.join(OBS_PATH, FINAL_OUTPUT_FILE), index=False)\n",
        "    ari_color_df = create_hex_colors(OBS_PATH, FINAL_OUTPUT_FILE)\n",
        "    ari_color_df.to_csv(os.path.join(OBS_PATH, FINAL_OUTPUT_FILE), index=False)\n",
        "    logger.info('All done updating %s with final output!', FINAL_OUTPUT_FILE)\n",
        "    print(\"All done updating \"+FINAL_OUTPUT_FILE+\" with final output\")\n",
        "    # This will upload the new .csv of precip/ari data to AGOL and overwrite the\n",
        "    # hosted feature layer.  If you for some reason need to add a new column to the\n",
        "    # .csv you will need to comment this line out and run the 'replace_gis_file'\n",
        "    # function below\n",
        "    do_some_gis(AGOL_USER, AGOL_PASSWORD, OBS_PATH, AGOL_URL, FINAL_OUTPUT_FILE)\n",
        "    logger.info('All done updating %s on the AGOL server', AGOL_URL)\n",
        "    print('All done updating %s on the AGOL server' %(AGOL_URL))\n",
        "    # Use the below line ONLY if you need to add a column to the final output .csv file\n",
        "    # This will delete the hosted feature layer and replace it with a new one\n",
        "    # You will need to comment out the 'do_some_gis' line above\n",
        "    #replace_gis_file(AGOL_USER, AGOL_PASSWORD, OBS_PATH, FINAL_OUTPUT_FILE, AGOL_NAME)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    try:\n",
        "        test_start = datetime.strptime(START, '%Y%m%d%H%M')\n",
        "        test_end = datetime.strptime(END, '%Y%m%d%H%M')\n",
        "        execute(START, END)\n",
        "    except ValueError:\n",
        "        test_start = 0\n",
        "        test_end = 0\n",
        "        print('Wrong datetime format! Must be YYYYmmddHHMM. Ex: 202012021800')\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1HrZBivODL0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iWvfnRZ0QHON"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "k6DRBRAXCyNh",
        "pNRHFuNmC-2K"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:33) \n[Clang 13.0.1 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "d9efab89797b4f7e4129f7fe7c375038c6a3f1b6c83da7efdea02c4da588d5be"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}